"""
AI Interviewer Services - AI ì±„ìš© ë©´ì ‘ê´€ ì„œë¹„ìŠ¤
"""
import json
import logging

# OpenAI import ì²˜ë¦¬
try:
    import openai
except ImportError:
    openai = None
    
# Anthropic import ì²˜ë¦¬
try:
    import anthropic
except ImportError:
    anthropic = None
import re
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from django.conf import settings
from django.utils import timezone
from django.core.cache import cache
from django.db.models import Q, Avg

from .models import (
    InterviewSession, InterviewQuestion, InterviewResponse,
    InterviewFeedback, InterviewTemplate, InterviewMetrics
)
from job_profiles.models import JobProfile
from employees.models import Employee

logger = logging.getLogger(__name__)


class AIInterviewer:
    """AI ë©´ì ‘ê´€ í´ë˜ìŠ¤"""
    
    def __init__(self, session: InterviewSession):
        self.session = session
        self.job_profile = session.job_profile
        self.ai_client = self._get_ai_client()
        self.personality = session.ai_personality
        self.language = session.interview_language
        
    def _get_ai_client(self):
        """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # OpenAI API ìš°ì„  ì‚¬ìš©
            if openai and hasattr(settings, 'OPENAI_API_KEY') and settings.OPENAI_API_KEY:
                openai.api_key = settings.OPENAI_API_KEY
                return 'openai'
            # Anthropic Claude ì‚¬ìš©
            elif anthropic and hasattr(settings, 'ANTHROPIC_API_KEY') and settings.ANTHROPIC_API_KEY:
                try:
                    # ìƒˆ ë²„ì „ ì‹œë„
                    return anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
                except (TypeError, AttributeError):
                    # êµ¬ë²„ì „ Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²˜ë¦¬
                    try:
                        return anthropic.Client(settings.ANTHROPIC_API_KEY)
                    except:
                        logger.warning("Anthropic client initialization failed")
                        return None
            else:
                logger.warning("AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
                return None
        except Exception as e:
            logger.error(f"AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return None
    
    def start_interview(self) -> Dict[str, Any]:
        """ë©´ì ‘ ì‹œì‘"""
        try:
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.session.status = 'IN_PROGRESS'
            self.session.started_at = timezone.now()
            self.session.save()
            
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ ìƒì„±
            first_question = self.generate_next_question()
            
            return {
                'success': True,
                'session_id': str(self.session.session_id),
                'status': 'STARTED',
                'first_question': self._format_question(first_question),
                'message': f"ì•ˆë…•í•˜ì„¸ìš”! {self.session.candidate_name}ë‹˜, {self.job_profile.title} ë©´ì ‘ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            logger.error(f"ë©´ì ‘ ì‹œì‘ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def generate_next_question(self, previous_response: Optional[InterviewResponse] = None) -> InterviewQuestion:
        """ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±"""
        try:
            question_number = self.session.questions.count() + 1
            
            # ì§ˆë¬¸ ìƒì„± ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_question_context(previous_response)
            
            # AIë¡œ ì§ˆë¬¸ ìƒì„±
            question_data = self._generate_question_with_ai(context, question_number)
            
            # ì§ˆë¬¸ ì €ì¥
            question = InterviewQuestion.objects.create(
                session=self.session,
                question_number=question_number,
                question_type=question_data.get('type', 'BEHAVIORAL'),
                question_text=question_data['text'],
                question_context=question_data.get('context', ''),
                expected_answer_points=question_data.get('expected_points', []),
                evaluation_criteria=question_data.get('criteria', 'COMMUNICATION'),
                difficulty_score=question_data.get('difficulty', 5.0),
                estimated_response_time=question_data.get('time_limit', 120),
                ai_reasoning=question_data.get('reasoning', ''),
                adapted_from_previous=bool(previous_response),
                tags=question_data.get('tags', [])
            )
            
            return question
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ì§ˆë¬¸ ìƒì„±
            return self._generate_fallback_question(question_number)
    
    def _build_question_context(self, previous_response: Optional[InterviewResponse] = None) -> Dict[str, Any]:
        """ì§ˆë¬¸ ìƒì„± ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context = {
            'job_profile': {
                'title': self.job_profile.title,
                'department': self.job_profile.department,
                'required_skills': self.job_profile.required_skills,
                'job_level': self.job_profile.job_level
            },
            'session_info': {
                'type': self.session.session_type,
                'difficulty': self.session.difficulty_level,
                'current_question': self.session.questions.count() + 1,
                'max_questions': self.session.max_questions
            },
            'candidate_info': {
                'name': self.session.candidate_name,
                'previous_responses': []
            }
        }
        
        # ì´ì „ ì‘ë‹µ ì •ë³´ ì¶”ê°€
        if previous_response:
            context['candidate_info']['previous_responses'] = [
                {
                    'question': previous_response.question.question_text,
                    'response': previous_response.response_text,
                    'score': previous_response.ai_score,
                    'analysis': previous_response.ai_analysis
                }
            ]
        
        return context
    
    def _generate_question_with_ai(self, context: Dict[str, Any], question_number: int) -> Dict[str, Any]:
        """AIë¥¼ ì‚¬ìš©í•´ ì§ˆë¬¸ ìƒì„±"""
        if not self.ai_client:
            return self._generate_simulated_question(question_number)
        
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = self._build_interviewer_prompt()
            user_prompt = f"""
ë‹¤ìŒ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì±„ìš© ê³µê³ : {context['job_profile']['title']} ({context['job_profile']['department']})
ë©´ì ‘ ìœ í˜•: {context['session_info']['type']}
ë‚œì´ë„: {context['session_info']['difficulty']}
ì§ˆë¬¸ ìˆœì„œ: {question_number}/{context['session_info']['max_questions']}

{f"ì´ì „ ì‘ë‹µ ë¶„ì„: {context['candidate_info']['previous_responses']}" if context['candidate_info']['previous_responses'] else "ì²« ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤."}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "text": "ì§ˆë¬¸ ë‚´ìš©",
    "type": "ì§ˆë¬¸ ìœ í˜• (BEHAVIORAL/TECHNICAL/SITUATIONAL ë“±)",
    "context": "ì§ˆë¬¸ ë°°ê²½ ì„¤ëª…",
    "expected_points": ["ê¸°ëŒ€í•˜ëŠ” ë‹µë³€ ìš”ì†Œë“¤"],
    "criteria": "í‰ê°€ ê¸°ì¤€ (COMMUNICATION/PROBLEM_SOLVING ë“±)",
    "difficulty": ë‚œì´ë„ ì ìˆ˜ (1-10),
    "time_limit": ì˜ˆìƒ ì‘ë‹µ ì‹œê°„(ì´ˆ),
    "reasoning": "ì´ ì§ˆë¬¸ì„ ì„ íƒí•œ ì´ìœ ",
    "tags": ["ê´€ë ¨ íƒœê·¸ë“¤"]
}}
"""
            
            if self.ai_client == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.session.ai_model_version,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )
                content = response.choices[0].message.content
                
            else:  # Anthropic Claude
                response = self.ai_client.messages.create(
                    model="claude-3-sonnet-20240229",
                    max_tokens=1000,
                    temperature=0.7,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                content = response.content[0].text
            
            # JSON íŒŒì‹±
            return json.loads(content)
            
        except Exception as e:
            logger.error(f"AI ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._generate_simulated_question(question_number)
    
    def _build_interviewer_prompt(self) -> str:
        """ë©´ì ‘ê´€ AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        personality_traits = {
            'PROFESSIONAL': 'ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸',
            'FRIENDLY': 'ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ',
            'CHALLENGING': 'ë„ì „ì ì´ê³  ë¶„ì„ì ì¸',
            'SUPPORTIVE': 'ì§€ì›ì ì´ê³  ê²©ë ¤í•˜ëŠ”'
        }
        
        personality = personality_traits.get(self.personality, 'ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸')
        
        return f"""ë‹¹ì‹ ì€ {personality} AI ë©´ì ‘ê´€ì…ë‹ˆë‹¤.

í•µì‹¬ ì—­í• :
1. ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì •í™•íˆ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ ìƒì„±
2. ì´ì „ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì ì‘í˜• í›„ì† ì§ˆë¬¸ ê°œë°œ
3. ì±„ìš© ê³µê³ ì™€ ì§ë¬´ì— íŠ¹í™”ëœ ë§ì¶¤í˜• ì§ˆë¬¸ êµ¬ì„±

ì§ˆë¬¸ ì›ì¹™:
- êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ ì¤‘ì‹¬ì 
- ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ íŒŒì•… ê°€ëŠ¥
- ì°¨ë³„ ì—†ì´ ê³µì •í•œ í‰ê°€ ê¸°ì¤€
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„
- ì ì ˆí•œ ë‚œì´ë„ì™€ ì‹œê°„ ë°°ë¶„

ë©´ì ‘ ì–¸ì–´: {self.language}
í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•˜ì„¸ìš”."""
    
    def _generate_simulated_question(self, question_number: int) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ì§ˆë¬¸ ìƒì„± (AI ì‚¬ìš© ë¶ˆê°€ì‹œ í´ë°±)"""
        question_templates = [
            {
                "text": f"{self.job_profile.title} ì§ë¬´ì— ì§€ì›í•˜ì‹  ì´ìœ ì™€ ë³¸ì¸ì´ ì´ ì—­í• ì— ì í•©í•œ ì´ìœ ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                "type": "BEHAVIORAL",
                "criteria": "COMMUNICATION",
                "difficulty": 3
            },
            {
                "text": "ì´ì „ ê²½í—˜ì—ì„œ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í•´ê²°í–ˆë˜ ì‚¬ë¡€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
                "type": "SITUATIONAL",
                "criteria": "PROBLEM_SOLVING",
                "difficulty": 5
            },
            {
                "text": "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ê°ˆë“±ì´ ìƒê²¼ì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ì‹œë‚˜ìš”?",
                "type": "BEHAVIORAL",
                "criteria": "TEAMWORK",
                "difficulty": 4
            },
            {
                "text": f"{self.job_profile.department}ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê°€ì¹˜ë‚˜ ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "type": "CULTURAL_FIT",
                "criteria": "CULTURAL_FIT",
                "difficulty": 3
            }
        ]
        
        template = question_templates[(question_number - 1) % len(question_templates)]
        
        return {
            "text": template["text"],
            "type": template["type"],
            "context": "AI ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ìƒì„±ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤.",
            "expected_points": ["êµ¬ì²´ì  ì‚¬ë¡€", "ë…¼ë¦¬ì  ì„¤ëª…", "ê²°ê³¼ ë° í•™ìŠµì "],
            "criteria": template["criteria"],
            "difficulty": template["difficulty"],
            "time_limit": 120,
            "reasoning": "ê¸°ë³¸ í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸",
            "tags": ["ê¸°ë³¸ì§ˆë¬¸", "ì‹œë®¬ë ˆì´ì…˜"]
        }
    
    def _generate_fallback_question(self, question_number: int) -> InterviewQuestion:
        """í´ë°± ì§ˆë¬¸ ìƒì„±"""
        question_data = self._generate_simulated_question(question_number)
        
        return InterviewQuestion.objects.create(
            session=self.session,
            question_number=question_number,
            question_type=question_data['type'],
            question_text=question_data['text'],
            question_context=question_data['context'],
            evaluation_criteria=question_data['criteria'],
            difficulty_score=question_data['difficulty'],
            estimated_response_time=120,
            generated_by_ai=False
        )
    
    def evaluate_response(self, response: InterviewResponse) -> Dict[str, Any]:
        """ì‘ë‹µ í‰ê°€"""
        try:
            if not self.ai_client:
                return self._evaluate_response_simulated(response)
            
            # AI í‰ê°€ ìˆ˜í–‰
            evaluation = self._evaluate_with_ai(response)
            
            # ì‘ë‹µ ì—…ë°ì´íŠ¸
            response.ai_score = evaluation['score']
            response.ai_feedback = evaluation['feedback']
            response.ai_analysis = evaluation['analysis']
            response.quality_rating = evaluation['quality_rating']
            response.sentiment_score = evaluation.get('sentiment_score', 0.0)
            response.confidence_level = evaluation.get('confidence_level', 0.0)
            response.clarity_score = evaluation.get('clarity_score', 0.0)
            response.relevance_score = evaluation.get('relevance_score', 0.0)
            response.keyword_matches = evaluation.get('keyword_matches', [])
            response.word_count = len(response.response_text.split())
            response.save()
            
            return {
                'success': True,
                'evaluation': evaluation,
                'next_question_ready': self._should_continue_interview()
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _evaluate_with_ai(self, response: InterviewResponse) -> Dict[str, Any]:
        """AIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ í‰ê°€"""
        try:
            question = response.question
            
            evaluation_prompt = f"""
ë‹¤ìŒ ë©´ì ‘ ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question.question_text}
í‰ê°€ ê¸°ì¤€: {question.get_evaluation_criteria_display()}
ì˜ˆìƒ ë‹µë³€ í¬ì¸íŠ¸: {question.expected_answer_points}

ì‘ë‹µì ë‹µë³€:
{response.response_text}

ì‘ë‹µ ì‹œê°„: {response.response_time_seconds:.1f}ì´ˆ
ì˜ˆìƒ ì‹œê°„: {question.estimated_response_time}ì´ˆ

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "score": 0-10 ì ìˆ˜,
    "feedback": "ìƒì„¸í•œ í”¼ë“œë°±",
    "analysis": {{
        "strengths": ["ê°•ì ë“¤"],
        "weaknesses": ["ì•½ì ë“¤"],
        "key_insights": ["í•µì‹¬ ì¸ì‚¬ì´íŠ¸"]
    }},
    "quality_rating": "EXCELLENT/GOOD/AVERAGE/BELOW_AVERAGE/POOR",
    "sentiment_score": -1.0 ~ 1.0 ê°ì • ì ìˆ˜,
    "confidence_level": 0.0 ~ 1.0 ìì‹ ê° ìˆ˜ì¤€,
    "clarity_score": 0.0 ~ 1.0 ëª…í™•ì„± ì ìˆ˜,
    "relevance_score": 0.0 ~ 1.0 ê´€ë ¨ì„± ì ìˆ˜,
    "keyword_matches": ["ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œë“¤"]
}}
"""
            
            if self.ai_client == 'openai':
                response = openai.ChatCompletion.create(
                    model=self.session.ai_model_version,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ê³µì •í•˜ê³  ì „ë¬¸ì ì¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": evaluation_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=800
                )
                content = response.choices[0].message.content
                
            else:  # Anthropic Claude
                response = self.ai_client.messages.create(
                    model="claude-3-sonnet-20240229",
                    max_tokens=800,
                    temperature=0.3,
                    system="ë‹¹ì‹ ì€ ê³µì •í•˜ê³  ì „ë¬¸ì ì¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.",
                    messages=[{"role": "user", "content": evaluation_prompt}]
                )
                content = response.content[0].text
            
            return json.loads(content)
            
        except Exception as e:
            logger.error(f"AI í‰ê°€ ì˜¤ë¥˜: {e}")
            return self._evaluate_response_simulated(response)
    
    def _evaluate_response_simulated(self, response: InterviewResponse) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ í‰ê°€"""
        word_count = len(response.response_text.split())
        time_factor = min(1.0, response.response_time_seconds / response.question.estimated_response_time)
        
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚° (ë‹¨ì–´ ìˆ˜ì™€ ì‹œê°„ ê¸°ë°˜)
        base_score = min(10.0, (word_count / 50) * 5 + time_factor * 3)
        
        return {
            'score': base_score,
            'feedback': 'ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œì—ì„œ ìƒì„±ëœ í‰ê°€ì…ë‹ˆë‹¤.',
            'analysis': {
                'strengths': ['ì‘ë‹µì„ ì œê³µí•¨'],
                'weaknesses': [],
                'key_insights': ['ê¸°ë³¸ì ì¸ ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥']
            },
            'quality_rating': 'AVERAGE',
            'sentiment_score': 0.0,
            'confidence_level': 0.5,
            'clarity_score': 0.6,
            'relevance_score': 0.7,
            'keyword_matches': []
        }
    
    def _should_continue_interview(self) -> bool:
        """ë©´ì ‘ ê³„ì† ì§„í–‰ ì—¬ë¶€ íŒë‹¨"""
        current_questions = self.session.questions.count()
        return current_questions < self.session.max_questions
    
    def complete_interview(self) -> Dict[str, Any]:
        """ë©´ì ‘ ì™„ë£Œ"""
        try:
            self.session.status = 'COMPLETED'
            self.session.completed_at = timezone.now()
            self.session.total_questions_asked = self.session.questions.count()
            self.session.total_responses = self.session.questions.filter(
                response__isnull=False
            ).count()
            
            # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            responses = InterviewResponse.objects.filter(
                question__session=self.session
            )
            if responses.exists():
                avg_time = responses.aggregate(
                    avg=Avg('response_time_seconds')
                )['avg']
                self.session.average_response_time = avg_time or 0.0
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚°
                avg_score = responses.aggregate(
                    avg=Avg('ai_score')
                )['avg']
                self.session.overall_score = avg_score or 0.0
            
            self.session.save()
            
            # ìµœì¢… AI í‰ê°€ ìƒì„±
            final_assessment = self._generate_final_assessment()
            self.session.ai_assessment = final_assessment
            self.session.save()
            
            # ë©´ì ‘ ì§€í‘œ ê³„ì‚°
            metrics = self._calculate_metrics()
            
            # í”¼ë“œë°± ìƒì„±
            feedback = self._generate_comprehensive_feedback()
            
            return {
                'success': True,
                'session_id': str(self.session.session_id),
                'status': 'COMPLETED',
                'final_score': self.session.overall_score,
                'duration_minutes': self.session.get_duration_minutes(),
                'assessment': final_assessment,
                'metrics': metrics,
                'feedback': feedback
            }
            
        except Exception as e:
            logger.error(f"ë©´ì ‘ ì™„ë£Œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _generate_final_assessment(self) -> Dict[str, Any]:
        """ìµœì¢… AI í‰ê°€ ìƒì„±"""
        responses = InterviewResponse.objects.filter(
            question__session=self.session
        )
        
        if not responses.exists():
            return {"message": "ì‘ë‹µì´ ì—†ì–´ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # ê¸°ë³¸ í†µê³„
        scores = [r.ai_score for r in responses if r.ai_score > 0]
        
        assessment = {
            'overall_performance': 'AVERAGE',
            'total_questions': self.session.questions.count(),
            'completed_responses': len(scores),
            'average_score': sum(scores) / len(scores) if scores else 0,
            'score_distribution': {
                'excellent': len([s for s in scores if s >= 8]),
                'good': len([s for s in scores if 6 <= s < 8]),
                'average': len([s for s in scores if 4 <= s < 6]),
                'below_average': len([s for s in scores if s < 4])
            },
            'key_strengths': [],
            'areas_for_improvement': [],
            'recommendation': 'NEUTRAL',
            'confidence': 0.7
        }
        
        # ì„±ê³¼ ë“±ê¸‰ ê²°ì •
        avg_score = assessment['average_score']
        if avg_score >= 8:
            assessment['overall_performance'] = 'EXCELLENT'
            assessment['recommendation'] = 'STRONGLY_RECOMMEND'
        elif avg_score >= 6:
            assessment['overall_performance'] = 'GOOD'
            assessment['recommendation'] = 'RECOMMEND'
        elif avg_score >= 4:
            assessment['overall_performance'] = 'AVERAGE'
            assessment['recommendation'] = 'NEUTRAL'
        else:
            assessment['overall_performance'] = 'BELOW_AVERAGE'
            assessment['recommendation'] = 'NOT_RECOMMEND'
        
        return assessment
    
    def _calculate_metrics(self) -> InterviewMetrics:
        """ë©´ì ‘ ì§€í‘œ ê³„ì‚°"""
        responses = InterviewResponse.objects.filter(
            question__session=self.session
        )
        
        metrics_data = {
            'total_duration_minutes': self.session.get_duration_minutes(),
            'average_question_time': 0.0,
            'response_completeness': 0.0,
            'average_response_length': 0,
            'communication_score': 0.0,
            'problem_solving_score': 0.0,
            'technical_competence': 0.0,
            'cultural_fit_score': 0.0
        }
        
        if responses.exists():
            # ì‹œê°„ ì§€í‘œ
            total_time = sum(r.response_time_seconds for r in responses)
            metrics_data['average_question_time'] = total_time / (responses.count() * 60)
            
            # ì‘ë‹µ í’ˆì§ˆ ì§€í‘œ
            response_lengths = [len(r.response_text.split()) for r in responses]
            metrics_data['average_response_length'] = sum(response_lengths) / len(response_lengths)
            metrics_data['response_completeness'] = min(1.0, metrics_data['average_response_length'] / 100)
            
            # ì—­ëŸ‰ë³„ ì ìˆ˜ (í‰ê°€ ê¸°ì¤€ë³„ë¡œ ì§‘ê³„)
            criteria_scores = {}
            for response in responses:
                criteria = response.question.evaluation_criteria
                if criteria not in criteria_scores:
                    criteria_scores[criteria] = []
                criteria_scores[criteria].append(response.ai_score)
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            for criteria, scores in criteria_scores.items():
                avg_score = sum(scores) / len(scores) / 10  # 0-1 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
                if criteria == 'COMMUNICATION':
                    metrics_data['communication_score'] = avg_score
                elif criteria == 'PROBLEM_SOLVING':
                    metrics_data['problem_solving_score'] = avg_score
                elif criteria == 'TECHNICAL_SKILLS':
                    metrics_data['technical_competence'] = avg_score
                elif criteria == 'CULTURAL_FIT':
                    metrics_data['cultural_fit_score'] = avg_score
        
        # ì¢…í•© ì¶”ì²œë„ ê²°ì •
        overall_score = sum([
            metrics_data['communication_score'],
            metrics_data['problem_solving_score'],
            metrics_data['technical_competence'],
            metrics_data['cultural_fit_score']
        ]) / 4
        
        if overall_score >= 0.8:
            recommendation = 'STRONGLY_RECOMMEND'
        elif overall_score >= 0.6:
            recommendation = 'RECOMMEND'
        elif overall_score >= 0.4:
            recommendation = 'NEUTRAL'
        elif overall_score >= 0.2:
            recommendation = 'NOT_RECOMMEND'
        else:
            recommendation = 'STRONGLY_NOT_RECOMMEND'
        
        metrics_data['overall_recommendation'] = recommendation
        
        # ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥
        metrics, created = InterviewMetrics.objects.update_or_create(
            session=self.session,
            defaults=metrics_data
        )
        
        return metrics
    
    def _generate_comprehensive_feedback(self) -> List[InterviewFeedback]:
        """ì¢…í•© í”¼ë“œë°± ìƒì„±"""
        feedback_list = []
        
        # ì¢…í•© í‰ê°€ í”¼ë“œë°±
        overall_feedback = InterviewFeedback.objects.create(
            session=self.session,
            feedback_type='OVERALL',
            title='ì¢…í•© ë©´ì ‘ í‰ê°€',
            content=f"""
{self.session.candidate_name}ë‹˜ì˜ ë©´ì ‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“Š ë©´ì ‘ ê²°ê³¼ ìš”ì•½:
â€¢ ì´ ì§ˆë¬¸ ìˆ˜: {self.session.total_questions_asked}ê°œ
â€¢ í‰ê·  ì ìˆ˜: {self.session.overall_score:.1f}/10ì   
â€¢ ì†Œìš” ì‹œê°„: {self.session.get_duration_minutes()}ë¶„
â€¢ ì™„ë£Œìœ¨: {self.session.get_completion_rate():.1f}%

ì „ë°˜ì ìœ¼ë¡œ ì„±ì‹¤í•˜ê²Œ ë©´ì ‘ì— ì„í•˜ì…¨ìœ¼ë©°, ê° ì§ˆë¬¸ì— ëŒ€í•´ ì§„ì†”í•œ ë‹µë³€ì„ í•´ì£¼ì…¨ìŠµë‹ˆë‹¤.
""",
            rating=max(1, min(5, int(self.session.overall_score / 2))),
            priority_level=5
        )
        feedback_list.append(overall_feedback)
        
        return feedback_list
    
    def _format_question(self, question: InterviewQuestion) -> Dict[str, Any]:
        """ì§ˆë¬¸ í¬ë§·íŒ…"""
        return {
            'id': question.id,
            'number': question.question_number,
            'type': question.get_question_type_display(),
            'text': question.question_text,
            'context': question.question_context,
            'time_limit': question.estimated_response_time,
            'difficulty': question.difficulty_score
        }


class InterviewTemplateManager:
    """ë©´ì ‘ í…œí”Œë¦¿ ê´€ë¦¬ì"""
    
    @staticmethod
    def create_default_templates():
        """ê¸°ë³¸ í…œí”Œë¦¿ ìƒì„±"""
        default_templates = [
            {
                'name': 'ì¼ë°˜ ì‹ ì…ì‚¬ì› ë©´ì ‘',
                'category': 'ENTRY_LEVEL',
                'description': 'ì‹ ì… ì‚¬ì›ì„ ìœ„í•œ ê¸°ë³¸ ë©´ì ‘ í…œí”Œë¦¿',
                'target_duration': 20,
                'question_count': 6,
                'difficulty_level': 'BEGINNER',
                'question_templates': [
                    {
                        'type': 'BEHAVIORAL',
                        'text': 'ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.',
                        'criteria': 'COMMUNICATION'
                    },
                    {
                        'type': 'BEHAVIORAL', 
                        'text': 'ìš°ë¦¬ íšŒì‚¬ì— ì§€ì›í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',
                        'criteria': 'CULTURAL_FIT'
                    }
                ]
            },
            {
                'name': 'ê¸°ìˆ ì§ ë©´ì ‘',
                'category': 'TECHNICAL',
                'description': 'ê°œë°œì ë° ê¸°ìˆ ì§ì„ ìœ„í•œ ë©´ì ‘ í…œí”Œë¦¿',
                'target_duration': 45,
                'question_count': 10,
                'difficulty_level': 'INTERMEDIATE',
                'question_templates': [
                    {
                        'type': 'TECHNICAL',
                        'text': 'ìµœê·¼ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.',
                        'criteria': 'TECHNICAL_SKILLS'
                    }
                ]
            }
        ]
        
        for template_data in default_templates:
            InterviewTemplate.objects.get_or_create(
                name=template_data['name'],
                defaults=template_data
            )


class InterviewAnalyzer:
    """ë©´ì ‘ ë¶„ì„ê¸°"""
    
    @staticmethod
    def get_session_statistics(days: int = 30) -> Dict[str, Any]:
        """ë©´ì ‘ ì„¸ì…˜ í†µê³„"""
        from django.db.models import Count, Avg
        from datetime import timedelta
        
        cutoff_date = timezone.now().date() - timedelta(days=days)
        sessions = InterviewSession.objects.filter(
            created_at__date__gte=cutoff_date
        )
        
        stats = {
            'total_sessions': sessions.count(),
            'completed_sessions': sessions.filter(status='COMPLETED').count(),
            'in_progress_sessions': sessions.filter(status='IN_PROGRESS').count(),
            'average_score': sessions.aggregate(avg=Avg('overall_score'))['avg'] or 0,
            'average_duration': sessions.aggregate(avg=Avg('average_response_time'))['avg'] or 0,
            'by_type': dict(sessions.values('session_type').annotate(count=Count('id')).values_list('session_type', 'count')),
            'by_status': dict(sessions.values('status').annotate(count=Count('id')).values_list('status', 'count')),
            'completion_rate': 0
        }
        
        if stats['total_sessions'] > 0:
            stats['completion_rate'] = (stats['completed_sessions'] / stats['total_sessions']) * 100
        
        return stats
    
    @staticmethod
    def get_candidate_insights(session_id: str) -> Dict[str, Any]:
        """í›„ë³´ì ì¸ì‚¬ì´íŠ¸ ë¶„ì„"""
        try:
            session = InterviewSession.objects.get(session_id=session_id)
            responses = InterviewResponse.objects.filter(question__session=session)
            
            insights = {
                'performance_trend': [],
                'strength_areas': [],
                'improvement_areas': [],
                'response_patterns': {},
                'recommendations': []
            }
            
            # ì„±ê³¼ íŠ¸ë Œë“œ ë¶„ì„
            for response in responses.order_by('question__question_number'):
                insights['performance_trend'].append({
                    'question': response.question.question_number,
                    'score': response.ai_score,
                    'time': response.response_time_seconds
                })
            
            # ê°•ì /ì•½ì  ë¶„ì„
            high_scores = responses.filter(ai_score__gte=7.0)
            low_scores = responses.filter(ai_score__lt=5.0)
            
            insights['strength_areas'] = [
                response.question.get_evaluation_criteria_display() 
                for response in high_scores
            ]
            
            insights['improvement_areas'] = [
                response.question.get_evaluation_criteria_display() 
                for response in low_scores
            ]
            
            return insights
            
        except InterviewSession.DoesNotExist:
            return {'error': 'ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}