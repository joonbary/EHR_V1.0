#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì§ë¬´ëª… ìë™ í‘œì¤€í™” ë° ìœ ì‚¬ëª…/ì˜¤íƒ€ ìë™ êµì • ì‹œìŠ¤í…œ
- Levenshtein ê±°ë¦¬, Jaro-Winkler ë“± ML ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­
- í‘œì¤€ ì§ë¬´ëª…ê³¼ ìë™ ë§¤í•‘ ë° êµì •
- ë¶ˆì¼ì¹˜/ìœ ì‚¬ì–´ ë¡œê¹… ë° ê´€ë¦¬ì ê²€ì¦
"""

import os
import sys
import io
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Set
import logging
from dataclasses import dataclass, asdict
import Levenshtein
from difflib import SequenceMatcher
import re
from tqdm import tqdm
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


@dataclass
class MappingResult:
    """ì§ë¬´ëª… ë§¤í•‘ ê²°ê³¼"""
    original: str
    standardized: str
    confidence: float
    method: str
    similarity_scores: Dict[str, float]
    is_exact_match: bool
    requires_review: bool
    suggestions: List[str]


class JobNameStandardizer:
    """ì§ë¬´ëª… í‘œì¤€í™” ì—”ì§„"""
    
    def __init__(self, standard_job_map: Dict[str, List[str]]):
        """
        Args:
            standard_job_map: í‘œì¤€ ì§ë¬´ëª… ë§¤í•‘ (ì§ì¢…: [ì§ë¬´ëª… ë¦¬ìŠ¤íŠ¸])
        """
        self.standard_job_map = standard_job_map
        self.all_standard_jobs = self._flatten_job_list()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ìœ ì‚¬ì–´/ë™ì˜ì–´ ì‚¬ì „
        self.synonym_map = {
            'ëª¨ê¸°ì§€ì‚¬ì—…': 'ëª¨ê¸°ì§€ê¸°íš',
            'ëª¨ê¸°ì§€ì˜ì—…': 'ëª¨ê¸°ì§€ê¸°íš',
            'NPLì˜ì—…ê¸°íš': 'NPLì‚¬ì—…ê¸°íš',
            'PLê¸°íš': 'ê°œì¸ì‹ ìš©ëŒ€ì¶œê¸°íš',
            'í”Œë«í¼/í•€í…Œí¬': 'ë””ì§€í„¸í”Œë«í¼',
            'ë°ì´í„°/í†µê³„': 'ë°ì´í„°ë¶„ì„',
            'HRM': 'ì¸ì‚¬ê´€ë¦¬',
            'HRD': 'ì¸ì¬ê°œë°œ',
            'PR': 'í™ë³´',
            'IBê¸ˆìœµ': 'íˆ¬ìê¸ˆìœµ',
            'ì—¬ì‹ ì˜ì—…': 'ê¸°ì—…ì—¬ì‹ ì˜ì—…',
            'ìˆ˜ì‹ ì˜ì—…': 'ì˜ˆê¸ˆì˜ì—…',
            'ìˆ˜ì‹ ê¸°íš': 'ì˜ˆê¸ˆê¸°íš',
            'ìˆ˜ì‹ ê³ ê°ì§€ì›': 'ì˜ˆê¸ˆê³ ê°ì§€ì›',
            'ì—¬ì‹ ê³ ê°ì§€ì›': 'ëŒ€ì¶œê³ ê°ì§€ì›',
            'ì±„ê¶Œê´€ë¦¬ì§€ì›': 'ì±„ê¶Œê´€ë¦¬',
            'ì‚¬ë¬´ì§€ì›': 'ì—…ë¬´ì§€ì›'
        }
        
        # ì§ë¬´ëª… ì •ê·œí™” íŒ¨í„´
        self.normalization_patterns = [
            (r'\s+', ''),  # ê³µë°± ì œê±°
            (r'[/Â·]', ''),  # íŠ¹ìˆ˜ë¬¸ì í†µì¼
            (r'íŒ€$', ''),  # 'íŒ€' ì ‘ë¯¸ì‚¬ ì œê±°
            (r'ë¶€$', ''),  # 'ë¶€' ì ‘ë¯¸ì‚¬ ì œê±°
            (r'ê´€ë¦¬ì$', 'ê´€ë¦¬'),  # ê´€ë¦¬ì -> ê´€ë¦¬
            (r'ë‹´ë‹¹ì?$', ''),  # ë‹´ë‹¹/ë‹´ë‹¹ì ì œê±°
        ]
    
    def _flatten_job_list(self) -> List[str]:
        """ëª¨ë“  í‘œì¤€ ì§ë¬´ëª…ì„ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        all_jobs = []
        for job_type, jobs in self.standard_job_map.items():
            all_jobs.extend(jobs)
        return list(set(all_jobs))
    
    def normalize_job_name(self, job_name: str) -> str:
        """ì§ë¬´ëª… ì •ê·œí™”"""
        normalized = job_name.strip()
        
        # ì •ê·œí™” íŒ¨í„´ ì ìš©
        for pattern, replacement in self.normalization_patterns:
            normalized = re.sub(pattern, replacement, normalized)
        
        return normalized
    
    def calculate_similarity(self, name1: str, name2: str) -> Dict[str, float]:
        """ë‹¤ì–‘í•œ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # ì •ê·œí™”
        norm1 = self.normalize_job_name(name1)
        norm2 = self.normalize_job_name(name2)
        
        # Levenshtein ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        lev_distance = Levenshtein.distance(norm1, norm2)
        max_len = max(len(norm1), len(norm2))
        lev_similarity = 1 - (lev_distance / max_len) if max_len > 0 else 0
        
        # Jaro-Winkler ìœ ì‚¬ë„
        jaro_winkler = Levenshtein.jaro_winkler(norm1, norm2)
        
        # SequenceMatcher ìœ ì‚¬ë„
        seq_similarity = SequenceMatcher(None, norm1, norm2).ratio()
        
        # í•œê¸€ ìëª¨ ë¶„í•´ í›„ ìœ ì‚¬ë„ (ë” ì •ë°€í•œ í•œê¸€ ë¹„êµ)
        jamo_similarity = self._calculate_jamo_similarity(norm1, norm2)
        
        # í† í° ê¸°ë°˜ ìœ ì‚¬ë„ (ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ)
        token_similarity = self._calculate_token_similarity(name1, name2)
        
        return {
            'levenshtein': lev_similarity,
            'jaro_winkler': jaro_winkler,
            'sequence': seq_similarity,
            'jamo': jamo_similarity,
            'token': token_similarity,
            'weighted_average': (
                lev_similarity * 0.2 +
                jaro_winkler * 0.3 +
                seq_similarity * 0.2 +
                jamo_similarity * 0.15 +
                token_similarity * 0.15
            )
        }
    
    def _calculate_jamo_similarity(self, str1: str, str2: str) -> float:
        """í•œê¸€ ìëª¨ ë¶„í•´ í›„ ìœ ì‚¬ë„ ê³„ì‚°"""
        def decompose_korean(text):
            result = []
            for char in text:
                if 'ê°€' <= char <= 'í£':
                    code = ord(char) - 0xAC00
                    jong = code % 28
                    jung = ((code - jong) // 28) % 21
                    cho = ((code - jong) // 28 - jung) // 21
                    result.extend([cho, jung, jong])
                else:
                    result.append(ord(char))
            return result
        
        jamo1 = decompose_korean(str1)
        jamo2 = decompose_korean(str2)
        
        if not jamo1 or not jamo2:
            return 0.0
        
        matches = sum(1 for a, b in zip(jamo1, jamo2) if a == b)
        return matches / max(len(jamo1), len(jamo2))
    
    def _calculate_token_similarity(self, str1: str, str2: str) -> float:
        """í† í° ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ë‹¨ì–´ ë¶„ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì ê¸°ì¤€)
        pattern = r'[ê°€-í£]+|[A-Za-z]+|[0-9]+'
        tokens1 = set(re.findall(pattern, str1.lower()))
        tokens2 = set(re.findall(pattern, str2.lower()))
        
        if not tokens1 or not tokens2:
            return 0.0
        
        intersection = tokens1 & tokens2
        union = tokens1 | tokens2
        
        return len(intersection) / len(union)
    
    def find_best_match(self, job_name: str) -> MappingResult:
        """ìµœì ì˜ í‘œì¤€ ì§ë¬´ëª… ë§¤ì¹­"""
        # 1. ë™ì˜ì–´ ì‚¬ì „ í™•ì¸
        if job_name in self.synonym_map:
            standardized = self.synonym_map[job_name]
            return MappingResult(
                original=job_name,
                standardized=standardized,
                confidence=1.0,
                method='synonym_dictionary',
                similarity_scores={'exact_synonym': 1.0},
                is_exact_match=True,
                requires_review=False,
                suggestions=[]
            )
        
        # 2. ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        if job_name in self.all_standard_jobs:
            return MappingResult(
                original=job_name,
                standardized=job_name,
                confidence=1.0,
                method='exact_match',
                similarity_scores={'exact': 1.0},
                is_exact_match=True,
                requires_review=False,
                suggestions=[]
            )
        
        # 3. ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
        best_matches = []
        for standard_job in self.all_standard_jobs:
            scores = self.calculate_similarity(job_name, standard_job)
            best_matches.append({
                'job': standard_job,
                'score': scores['weighted_average'],
                'scores': scores
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        best_matches.sort(key=lambda x: x['score'], reverse=True)
        top_matches = best_matches[:5]
        
        # ìµœê³  ë§¤ì¹˜ ì„ íƒ
        best = top_matches[0]
        confidence = best['score']
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ì— ë”°ë¥¸ ì²˜ë¦¬
        if confidence >= 0.9:
            method = 'high_confidence_match'
            requires_review = False
        elif confidence >= 0.7:
            method = 'medium_confidence_match'
            requires_review = True
        else:
            method = 'low_confidence_match'
            requires_review = True
        
        return MappingResult(
            original=job_name,
            standardized=best['job'] if confidence >= 0.7 else job_name,
            confidence=confidence,
            method=method,
            similarity_scores=best['scores'],
            is_exact_match=False,
            requires_review=requires_review,
            suggestions=[m['job'] for m in top_matches]
        )


class JobProfileStandardizer:
    """ì§ë¬´ í”„ë¡œí•„ í‘œì¤€í™” ì²˜ë¦¬ê¸°"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # í‘œì¤€ ì§ë¬´ëª… ì •ì˜ (OKê¸ˆìœµê·¸ë£¹ ê¸°ì¤€)
        self.standard_jobs = {
            'ITê¸°íš': ['ì‹œìŠ¤í…œê¸°íš', 'ì •ë³´ì „ëµê¸°íš', 'ITê¸°íš'],
            'ITê°œë°œ': ['ì‹œìŠ¤í…œê°œë°œ', 'ì• í”Œë¦¬ì¼€ì´ì…˜ê°œë°œ', 'ì†”ë£¨ì…˜ê°œë°œ'],
            'ITìš´ì˜': ['ì‹œìŠ¤í…œê´€ë¦¬', 'ì„œë¹„ìŠ¤ìš´ì˜', 'ITìš´ì˜ê´€ë¦¬', 'ì¸í”„ë¼ê´€ë¦¬'],
            'ê²½ì˜ê´€ë¦¬': [
                'ê°ì‚¬', 'ì¸ì‚¬ê´€ë¦¬', 'ì¸ì¬ê°œë°œ', 'ê²½ì˜ì§€ì›', 'ë¹„ì„œ', 'í™ë³´',
                'ê²½ì˜ê¸°íš', 'ë””ìì¸', 'ë¦¬ìŠ¤í¬ê´€ë¦¬', 'ë§ˆì¼€íŒ…', 'ìŠ¤í¬ì¸ ì‚¬ë¬´ê´€ë¦¬',
                'ìê¸ˆ', 'ì¬ë¬´íšŒê³„', 'ì •ë³´ë³´ì•ˆ', 'ì¤€ë²•ì§€ì›', 'ì´ë¬´'
            ],
            'íˆ¬ìê¸ˆìœµ': ['íˆ¬ìê¸ˆìœµ', 'IBì—…ë¬´', 'ê¸°ì—…ê¸ˆìœµìë¬¸'],
            'ê¸°ì—…ê¸ˆìœµ': ['ê¸°ì—…ì˜ì—…ê¸°íš', 'ê¸°ì—…ì—¬ì‹ ì‹¬ì‚¬', 'ê¸°ì—…ì—¬ì‹ ê´€ë¦¬'],
            'ê¸°ì—…ì˜ì—…': ['ê¸°ì—…ì—¬ì‹ ì˜ì—…', 'ê¸°ì—…ê³ ê°ê´€ë¦¬'],
            'ë¦¬í…Œì¼ê¸ˆìœµ': [
                'ë°ì´í„°ë¶„ì„', 'ë””ì§€í„¸í”Œë«í¼', 'NPLì‚¬ì—…ê¸°íš', 'ë¦¬í…Œì¼ì‹¬ì‚¬ê¸°íš',
                'ê°œì¸ì‹ ìš©ëŒ€ì¶œê¸°íš', 'ëª¨ê¸°ì§€ê¸°íš', 'ì˜ˆê¸ˆê¸°íš', 'ì˜ˆê¸ˆì˜ì—…'
            ],
            'ê³ ê°ì§€ì›': [
                'ëŒ€ì¶œê³ ê°ì§€ì›', 'ì—…ë¬´ì§€ì›', 'ì˜ˆê¸ˆê³ ê°ì§€ì›', 'ì±„ê¶Œê´€ë¦¬'
            ]
        }
        
        self.standardizer = JobNameStandardizer(self.standard_jobs)
        self.mapping_results = []
    
    def load_job_profiles(self, excel_path: str = None, json_path: str = None) -> pd.DataFrame:
        """ì§ë¬´ í”„ë¡œí•„ ë°ì´í„° ë¡œë“œ"""
        if excel_path and os.path.exists(excel_path):
            self.logger.info(f"Excel íŒŒì¼ ë¡œë“œ: {excel_path}")
            df = pd.read_excel(excel_path, sheet_name='ì „ì²´ì§ë¬´(37ê°œ)')
            return df
        elif json_path and os.path.exists(json_path):
            self.logger.info(f"JSON íŒŒì¼ ë¡œë“œ: {json_path}")
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return pd.DataFrame(data.get('jobs', []))
        else:
            raise FileNotFoundError("Excel ë˜ëŠ” JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def standardize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ ì§ë¬´ëª… í‘œì¤€í™”"""
        self.logger.info("ì§ë¬´ëª… í‘œì¤€í™” ì‹œì‘...")
        
        # í‘œì¤€í™” ê²°ê³¼ ì €ì¥
        standardized_df = df.copy()
        self.mapping_results = []
        
        # ì§ë¬´ëª… ì»¬ëŸ¼ í™•ì¸
        job_column = None
        for col in ['ì§ë¬´ëª…', 'ì§ë¬´', 'job_title']:
            if col in df.columns:
                job_column = col
                break
        
        if not job_column:
            raise ValueError("ì§ë¬´ëª… ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê° ì§ë¬´ëª… í‘œì¤€í™”
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="ì§ë¬´ëª… í‘œì¤€í™”"):
            original_name = row[job_column]
            if pd.isna(original_name):
                continue
            
            # í‘œì¤€í™” ìˆ˜í–‰
            result = self.standardizer.find_best_match(original_name)
            self.mapping_results.append(result)
            
            # ê²°ê³¼ ì ìš©
            standardized_df.at[idx, f'{job_column}_ì›ë³¸'] = original_name
            standardized_df.at[idx, job_column] = result.standardized
            standardized_df.at[idx, 'í‘œì¤€í™”_ì‹ ë¢°ë„'] = result.confidence
            standardized_df.at[idx, 'í‘œì¤€í™”_ë°©ë²•'] = result.method
            standardized_df.at[idx, 'ê²€í† í•„ìš”'] = 'Y' if result.requires_review else 'N'
            
            # ë¡œê¹…
            if result.requires_review:
                self.logger.warning(
                    f"ê²€í†  í•„ìš”: '{original_name}' -> '{result.standardized}' "
                    f"(ì‹ ë¢°ë„: {result.confidence:.2f})"
                )
        
        return standardized_df
    
    def generate_mapping_report(self) -> pd.DataFrame:
        """ë§¤í•‘ ê²°ê³¼ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_data = []
        
        for result in self.mapping_results:
            report_data.append({
                'ì›ë³¸ ì§ë¬´ëª…': result.original,
                'í‘œì¤€ ì§ë¬´ëª…': result.standardized,
                'ì‹ ë¢°ë„': f"{result.confidence:.2%}",
                'ë§¤ì¹­ ë°©ë²•': result.method,
                'ì •í™• ì¼ì¹˜': 'Y' if result.is_exact_match else 'N',
                'ê²€í†  í•„ìš”': 'Y' if result.requires_review else 'N',
                'Levenshtein ìœ ì‚¬ë„': f"{result.similarity_scores.get('levenshtein', 0):.2%}",
                'Jaro-Winkler ìœ ì‚¬ë„': f"{result.similarity_scores.get('jaro_winkler', 0):.2%}",
                'ì¶”ì²œ í›„ë³´ 1': result.suggestions[0] if len(result.suggestions) > 0 else '',
                'ì¶”ì²œ í›„ë³´ 2': result.suggestions[1] if len(result.suggestions) > 1 else '',
                'ì¶”ì²œ í›„ë³´ 3': result.suggestions[2] if len(result.suggestions) > 2 else ''
            })
        
        return pd.DataFrame(report_data)
    
    def export_results(self, standardized_df: pd.DataFrame, mapping_report_df: pd.DataFrame):
        """í‘œì¤€í™” ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. í‘œì¤€í™”ëœ Excel ì €ì¥
        excel_dir = os.path.join(self.output_dir, 'standardized')
        os.makedirs(excel_dir, exist_ok=True)
        
        excel_path = os.path.join(excel_dir, f'ì§ë¬´í”„ë¡œí•„_í‘œì¤€í™”_{timestamp}.xlsx')
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # í‘œì¤€í™” ë°ì´í„°
            standardized_df.to_excel(writer, sheet_name='í‘œì¤€í™”_ì§ë¬´í”„ë¡œí•„', index=False)
            
            # ë§¤í•‘ ë¦¬í¬íŠ¸
            mapping_report_df.to_excel(writer, sheet_name='ë§¤í•‘_ìƒì„¸ê²°ê³¼', index=False)
            
            # ê²€í†  í•„ìš” í•­ëª©
            review_needed = mapping_report_df[mapping_report_df['ê²€í†  í•„ìš”'] == 'Y']
            if not review_needed.empty:
                review_needed.to_excel(writer, sheet_name='ê²€í† í•„ìš”í•­ëª©', index=False)
            
            # í†µê³„ ìš”ì•½
            stats_df = self._generate_statistics(standardized_df, mapping_report_df)
            stats_df.to_excel(writer, sheet_name='í‘œì¤€í™”_í†µê³„', index=False)
        
        # Excel ìŠ¤íƒ€ì¼ ì ìš©
        self._apply_excel_styles(excel_path)
        
        # 2. JSON ì €ì¥
        json_dir = os.path.join(self.output_dir, 'json')
        os.makedirs(json_dir, exist_ok=True)
        
        json_path = os.path.join(json_dir, f'standardized_profiles_{timestamp}.json')
        
        json_data = {
            'metadata': {
                'standardized_at': datetime.now().isoformat(),
                'total_jobs': len(standardized_df),
                'exact_matches': len([r for r in self.mapping_results if r.is_exact_match]),
                'review_needed': len([r for r in self.mapping_results if r.requires_review])
            },
            'profiles': standardized_df.to_dict('records'),
            'mapping_results': [asdict(r) for r in self.mapping_results]
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # 3. ë§¤í•‘ í…Œì´ë¸” CSV
        mapping_path = os.path.join(self.output_dir, f'job_mapping_table_{timestamp}.csv')
        mapping_report_df.to_csv(mapping_path, encoding='utf-8-sig', index=False)
        
        # 4. ë¡œê·¸ íŒŒì¼
        log_dir = os.path.join(self.output_dir, 'logs')
        os.makedirs(log_dir, exist_ok=True)
        
        log_path = os.path.join(log_dir, f'standardization_log_{timestamp}.txt')
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write(f"ì§ë¬´ëª… í‘œì¤€í™” ë¡œê·¸\n")
            f.write(f"ì²˜ë¦¬ ì¼ì‹œ: {datetime.now()}\n")
            f.write(f"ì´ ì²˜ë¦¬ ê±´ìˆ˜: {len(self.mapping_results)}\n\n")
            
            f.write("=== ê²€í†  í•„ìš” í•­ëª© ===\n")
            for result in self.mapping_results:
                if result.requires_review:
                    f.write(f"'{result.original}' -> '{result.standardized}' "
                           f"(ì‹ ë¢°ë„: {result.confidence:.2%})\n")
        
        return excel_path, json_path, mapping_path, log_path
    
    def _generate_statistics(self, standardized_df: pd.DataFrame, 
                           mapping_report_df: pd.DataFrame) -> pd.DataFrame:
        """í‘œì¤€í™” í†µê³„ ìƒì„±"""
        stats = []
        
        # ì „ì²´ í†µê³„
        total = len(self.mapping_results)
        exact_matches = len([r for r in self.mapping_results if r.is_exact_match])
        high_confidence = len([r for r in self.mapping_results if r.confidence >= 0.9])
        medium_confidence = len([r for r in self.mapping_results if 0.7 <= r.confidence < 0.9])
        low_confidence = len([r for r in self.mapping_results if r.confidence < 0.7])
        review_needed = len([r for r in self.mapping_results if r.requires_review])
        
        stats.append({
            'í•­ëª©': 'ì „ì²´ ì§ë¬´',
            'ê±´ìˆ˜': total,
            'ë¹„ìœ¨': '100%'
        })
        stats.append({
            'í•­ëª©': 'ì •í™• ì¼ì¹˜',
            'ê±´ìˆ˜': exact_matches,
            'ë¹„ìœ¨': f"{exact_matches/total*100:.1f}%"
        })
        stats.append({
            'í•­ëª©': 'ë†’ì€ ì‹ ë¢°ë„ (90% ì´ìƒ)',
            'ê±´ìˆ˜': high_confidence,
            'ë¹„ìœ¨': f"{high_confidence/total*100:.1f}%"
        })
        stats.append({
            'í•­ëª©': 'ì¤‘ê°„ ì‹ ë¢°ë„ (70-90%)',
            'ê±´ìˆ˜': medium_confidence,
            'ë¹„ìœ¨': f"{medium_confidence/total*100:.1f}%"
        })
        stats.append({
            'í•­ëª©': 'ë‚®ì€ ì‹ ë¢°ë„ (70% ë¯¸ë§Œ)',
            'ê±´ìˆ˜': low_confidence,
            'ë¹„ìœ¨': f"{low_confidence/total*100:.1f}%"
        })
        stats.append({
            'í•­ëª©': 'ê²€í†  í•„ìš”',
            'ê±´ìˆ˜': review_needed,
            'ë¹„ìœ¨': f"{review_needed/total*100:.1f}%"
        })
        
        # ë§¤ì¹­ ë°©ë²•ë³„ í†µê³„
        method_counts = {}
        for result in self.mapping_results:
            method = result.method
            method_counts[method] = method_counts.get(method, 0) + 1
        
        for method, count in method_counts.items():
            stats.append({
                'í•­ëª©': f'ë§¤ì¹­ ë°©ë²•: {method}',
                'ê±´ìˆ˜': count,
                'ë¹„ìœ¨': f"{count/total*100:.1f}%"
            })
        
        return pd.DataFrame(stats)
    
    def _apply_excel_styles(self, excel_path: str):
        """Excel ìŠ¤íƒ€ì¼ ì ìš©"""
        wb = openpyxl.load_workbook(excel_path)
        
        # ìŠ¤íƒ€ì¼ ì •ì˜
        header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
        header_font = Font(color='FFFFFF', bold=True)
        review_fill = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid')
        low_conf_fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')
        
        for sheet_name in wb.sheetnames:
            ws = wb[sheet_name]
            
            # í—¤ë” ìŠ¤íƒ€ì¼
            for cell in ws[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal='center', vertical='center')
            
            # ì¡°ê±´ë¶€ ì„œì‹
            if sheet_name in ['í‘œì¤€í™”_ì§ë¬´í”„ë¡œí•„', 'ë§¤í•‘_ìƒì„¸ê²°ê³¼']:
                for row in ws.iter_rows(min_row=2):
                    # ê²€í†  í•„ìš” í•­ëª© í•˜ì´ë¼ì´íŠ¸
                    for cell in row:
                        if cell.column_letter in ['F', 'G']:  # ê²€í† í•„ìš” ì»¬ëŸ¼
                            if cell.value == 'Y':
                                for c in row:
                                    c.fill = review_fill
                        
                        # ë‚®ì€ ì‹ ë¢°ë„ í•˜ì´ë¼ì´íŠ¸
                        if 'ì‹ ë¢°ë„' in str(ws.cell(1, cell.column).value):
                            try:
                                conf_value = float(str(cell.value).rstrip('%')) / 100
                                if conf_value < 0.7:
                                    cell.fill = low_conf_fill
                            except:
                                pass
            
            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
            for column in ws.columns:
                max_length = 0
                column_letter = get_column_letter(column[0].column)
                
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                
                adjusted_width = min(max_length + 2, 50)
                ws.column_dimensions[column_letter].width = adjusted_width
        
        wb.save(excel_path)


def generate_sample_code():
    """ìƒ˜í”Œ ì‚¬ìš© ì½”ë“œ ìƒì„±"""
    sample_code = """
# ì§ë¬´ëª… í‘œì¤€í™” ì‚¬ìš© ì˜ˆì œ

from job_profile_name_standardize import JobProfileStandardizer, JobNameStandardizer

# 1. ê¸°ë³¸ ì‚¬ìš©ë²•
standardizer = JobProfileStandardizer(output_dir='./output')

# Excel íŒŒì¼ì—ì„œ ë¡œë“œ ë° í‘œì¤€í™”
df = standardizer.load_job_profiles(
    excel_path='./job_profile_complete/excel/OKê¸ˆìœµê·¸ë£¹_ì§ë¬´ê¸°ìˆ ì„œ_ì „ì²´.xlsx'
)
standardized_df = standardizer.standardize_dataframe(df)

# ê²°ê³¼ ì €ì¥
mapping_report = standardizer.generate_mapping_report()
excel_path, json_path, mapping_path, log_path = standardizer.export_results(
    standardized_df, mapping_report
)

# 2. ê°œë³„ ì§ë¬´ëª… í‘œì¤€í™”
job_standardizer = JobNameStandardizer({
    'ITê¸°íš': ['ì‹œìŠ¤í…œê¸°íš', 'ITê¸°íš'],
    'ITê°œë°œ': ['ì‹œìŠ¤í…œê°œë°œ', 'ì• í”Œë¦¬ì¼€ì´ì…˜ê°œë°œ']
})

# ë‹¨ì¼ ì§ë¬´ëª… ë§¤ì¹­
result = job_standardizer.find_best_match('ì‹œìŠ¤í…œ ê¸°íš')
print(f"ì›ë³¸: {result.original}")
print(f"í‘œì¤€í™”: {result.standardized}")
print(f"ì‹ ë¢°ë„: {result.confidence:.2%}")
print(f"ê²€í† í•„ìš”: {result.requires_review}")

# 3. ìœ ì‚¬ë„ ê³„ì‚°
scores = job_standardizer.calculate_similarity('ëª¨ê¸°ì§€ì‚¬ì—…', 'ëª¨ê¸°ì§€ê¸°íš')
print(f"Levenshtein: {scores['levenshtein']:.2%}")
print(f"Jaro-Winkler: {scores['jaro_winkler']:.2%}")

# 4. ë°°ì¹˜ ì²˜ë¦¬
job_names = ['HRM', 'HRD', 'í”Œë«í¼/í•€í…Œí¬', 'IBê¸ˆìœµ']
results = [job_standardizer.find_best_match(name) for name in job_names]

# 5. Django ëª¨ë¸ í†µí•© ì˜ˆì œ
from job_profiles.models import JobProfile

# í‘œì¤€í™” í›„ DB ì—…ë°ì´íŠ¸
for index, row in standardized_df.iterrows():
    try:
        job_profile = JobProfile.objects.get(id=row['id'])
        job_profile.job_title = row['ì§ë¬´ëª…']  # í‘œì¤€í™”ëœ ì§ë¬´ëª…
        job_profile.original_title = row['ì§ë¬´ëª…_ì›ë³¸']  # ì›ë³¸ ë³´ê´€
        job_profile.save()
    except JobProfile.DoesNotExist:
        pass
"""
    
    return sample_code


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì •
    excel_path = r"C:/Users/apro/OneDrive/Desktop/ì„¤ëª…íšŒìë£Œ/job_profile_complete/excel/OKê¸ˆìœµê·¸ë£¹_ì§ë¬´ê¸°ìˆ ì„œ_ì „ì²´_20250726_183527.xlsx"
    json_path = r"C:/Users/apro/OneDrive/Desktop/ì„¤ëª…íšŒìë£Œ/job_profile_complete/json/ok_job_profiles_complete.json"
    output_dir = r"C:/Users/apro/OneDrive/Desktop/ì„¤ëª…íšŒìë£Œ/job_profile_standardized"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(output_dir, 'standardization.log')
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("="*60)
    logger.info("ì§ë¬´ëª… ìë™ í‘œì¤€í™” ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("="*60)
    
    try:
        # í‘œì¤€í™” ì²˜ë¦¬ê¸° ìƒì„±
        standardizer = JobProfileStandardizer(output_dir)
        
        # ë°ì´í„° ë¡œë“œ
        df = standardizer.load_job_profiles(excel_path=excel_path)
        logger.info(f"ì´ {len(df)}ê°œ ì§ë¬´ í”„ë¡œí•„ ë¡œë“œ ì™„ë£Œ")
        
        # í‘œì¤€í™” ìˆ˜í–‰
        standardized_df = standardizer.standardize_dataframe(df)
        
        # ë§¤í•‘ ë¦¬í¬íŠ¸ ìƒì„±
        mapping_report = standardizer.generate_mapping_report()
        
        # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        excel_out, json_out, mapping_out, log_out = standardizer.export_results(
            standardized_df, mapping_report
        )
        
        # í†µê³„ ì¶œë ¥
        total = len(standardizer.mapping_results)
        exact = len([r for r in standardizer.mapping_results if r.is_exact_match])
        review = len([r for r in standardizer.mapping_results if r.requires_review])
        
        logger.info("="*60)
        logger.info("í‘œì¤€í™” ì™„ë£Œ!")
        logger.info(f"ì´ ì²˜ë¦¬: {total}ê°œ")
        logger.info(f"ì •í™• ì¼ì¹˜: {exact}ê°œ ({exact/total*100:.1f}%)")
        logger.info(f"ê²€í†  í•„ìš”: {review}ê°œ ({review/total*100:.1f}%)")
        logger.info("="*60)
        
        print(f"\nâœ… ì§ë¬´ëª… í‘œì¤€í™” ì™„ë£Œ!")
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {total}ê°œ ì§ë¬´ëª… í‘œì¤€í™”")
        print(f"ğŸ¯ ì •í™• ì¼ì¹˜: {exact}ê°œ")
        print(f"âš ï¸  ê²€í†  í•„ìš”: {review}ê°œ")
        print(f"\nğŸ“ ì¶œë ¥ íŒŒì¼:")
        print(f"  - Excel: {os.path.basename(excel_out)}")
        print(f"  - JSON: {os.path.basename(json_out)}")
        print(f"  - ë§¤í•‘í‘œ: {os.path.basename(mapping_out)}")
        print(f"  - ë¡œê·¸: {os.path.basename(log_out)}")
        
        # ìƒ˜í”Œ ì½”ë“œ ìƒì„±
        sample_code_path = os.path.join(output_dir, 'sample_usage.py')
        with open(sample_code_path, 'w', encoding='utf-8') as f:
            f.write(generate_sample_code())
        print(f"  - ìƒ˜í”Œ ì½”ë“œ: {os.path.basename(sample_code_path)}")
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == '__main__':
    main()