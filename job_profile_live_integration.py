#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
í‘œì¤€ ì§ë¬´ëª… DBì™€ AIì§ë¬´ì±—ë´‡Â·ì§ë¬´ê²€ìƒ‰Â·ë¶„ì„ì‹œìŠ¤í…œ ì‹¤ì‹œê°„ í†µí•©
- ì‹¤ì‹œê°„ ì–‘ë°©í–¥ ì—°ë™ ë° ìë™ ì •í•©í™”
- Synonym dictionary, Fuzzy matching
- API/DB Viewë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì§ˆì˜/ë¶„ì„
"""

import os
import sys
import io
import json
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import sqlite3
import pandas as pd
from django.db import models, transaction
from django.core.cache import cache
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.utils.decorators import method_decorator
from django.views import View
# import redis
# from celery import Celery
# import Levenshtein
from difflib import SequenceMatcher
import re
# from tqdm import tqdm
# import openai
# from elasticsearch import Elasticsearch
# import numpy as np
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


@dataclass
class JobMatchResult:
    """ì§ë¬´ ë§¤ì¹­ ê²°ê³¼"""
    query: str
    matched_job: Dict[str, Any]
    confidence: float
    method: str
    alternatives: List[Dict[str, Any]]
    processing_time: float
    cache_hit: bool


@dataclass
class SyncResult:
    """ë™ê¸°í™” ê²°ê³¼"""
    operation: str
    affected_records: int
    success: bool
    error_message: Optional[str]
    timestamp: datetime


class JobProfileStandardizer:
    """ì‹¤ì‹œê°„ ì§ë¬´ëª… í‘œì¤€í™” ì—”ì§„"""
    
    def __init__(self, redis_client=None):
        # self.redis_client = redis_client or redis.Redis(host='localhost', port=6379, db=0)
        self.redis_client = redis_client  # Mock for demo
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # í‘œì¤€ ì§ë¬´ëª… ìºì‹œ í‚¤
        self.CACHE_KEY_JOBS = "standard_jobs"
        self.CACHE_KEY_SYNONYMS = "job_synonyms"
        self.CACHE_TTL = 3600  # 1ì‹œê°„
        
        # TF-IDF ë²¡í„°ë¼ì´ì € (ê²€ìƒ‰ ê°œì„ ) - Mock for demo
        # self.vectorizer = TfidfVectorizer(
        #     analyzer='char',
        #     ngram_range=(1, 3),
        #     max_features=1000
        # )
        self.job_vectors = None
        self.job_names = []
        
        self._initialize_cache()
    
    def _initialize_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        try:
            # í‘œì¤€ ì§ë¬´ëª… ë¡œë“œ
            standard_jobs = self._load_standard_jobs_from_db()
            self.redis_client.setex(
                self.CACHE_KEY_JOBS, 
                self.CACHE_TTL, 
                json.dumps(standard_jobs, ensure_ascii=False)
            )
            
            # ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ
            synonyms = self._load_synonyms_from_db()
            self.redis_client.setex(
                self.CACHE_KEY_SYNONYMS,
                self.CACHE_TTL,
                json.dumps(synonyms, ensure_ascii=False)
            )
            
            # TF-IDF ë²¡í„° ìƒì„±
            self._build_tfidf_vectors(standard_jobs)
            
            self.logger.info("ì§ë¬´ëª… í‘œì¤€í™” ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_standard_jobs_from_db(self) -> List[Dict]:
        """DBì—ì„œ í‘œì¤€ ì§ë¬´ëª… ë¡œë“œ"""
        from job_profiles.models import JobProfile
        
        jobs = JobProfile.objects.filter(is_active=True).values(
            'id', 'job_title', 'job_type', 'job_category', 
            'job_code', 'description', 'requirements'
        )
        return list(jobs)
    
    def _load_synonyms_from_db(self) -> Dict[str, str]:
        """DBì—ì„œ ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ"""
        # JobSynonym ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •
        try:
            from job_profiles.models import JobSynonym
            synonyms = JobSynonym.objects.filter(is_active=True).values_list(
                'synonym', 'standard_name'
            )
            return dict(synonyms)
        except:
            # ê¸°ë³¸ ë™ì˜ì–´ ì‚¬ì „
            return {
                'HRM': 'ì¸ì‚¬ê´€ë¦¬',
                'HRD': 'ì¸ì¬ê°œë°œ',
                'PR': 'í™ë³´',
                'IBê¸ˆìœµ': 'íˆ¬ìê¸ˆìœµ',
                'í”Œë«í¼/í•€í…Œí¬': 'ë””ì§€í„¸í”Œë«í¼',
                'ë°ì´í„°/í†µê³„': 'ë°ì´í„°ë¶„ì„',
                'ëª¨ê¸°ì§€ì‚¬ì—…': 'ëª¨ê¸°ì§€ê¸°íš',
                'PLê¸°íš': 'ê°œì¸ì‹ ìš©ëŒ€ì¶œê¸°íš'
            }
    
    def _build_tfidf_vectors(self, jobs: List[Dict]):
        """TF-IDF ë²¡í„° êµ¬ì¶•"""
        self.job_names = [job['job_title'] for job in jobs]
        if self.job_names:
            self.job_vectors = self.vectorizer.fit_transform(self.job_names)
    
    def normalize_query(self, query: str) -> str:
        """ê²€ìƒ‰ì–´ ì •ê·œí™”"""
        # ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = re.sub(r'[\s\-_/]', '', query.strip())
        # ì˜ì–´ ì†Œë¬¸ì ë³€í™˜
        normalized = normalized.lower()
        return normalized
    
    async def find_best_match(self, query: str) -> JobMatchResult:
        """ìµœì  ì§ë¬´ ë§¤ì¹­ (ë¹„ë™ê¸°)"""
        start_time = datetime.now()
        cache_key = f"job_match:{query}"
        
        # ìºì‹œ í™•ì¸
        cached_result = self.redis_client.get(cache_key)
        if cached_result:
            result = json.loads(cached_result)
            result['cache_hit'] = True
            result['processing_time'] = (datetime.now() - start_time).total_seconds()
            return JobMatchResult(**result)
        
        # í‘œì¤€ ì§ë¬´ëª… ë¡œë“œ
        jobs_data = self.redis_client.get(self.CACHE_KEY_JOBS)
        synonyms_data = self.redis_client.get(self.CACHE_KEY_SYNONYMS)
        
        if not jobs_data:
            self._initialize_cache()
            jobs_data = self.redis_client.get(self.CACHE_KEY_JOBS)
        
        jobs = json.loads(jobs_data) if jobs_data else []
        synonyms = json.loads(synonyms_data) if synonyms_data else {}
        
        # 1. ë™ì˜ì–´ ì‚¬ì „ í™•ì¸
        normalized_query = self.normalize_query(query)
        if normalized_query in synonyms:
            standard_name = synonyms[normalized_query]
            matched_job = next((job for job in jobs if job['job_title'] == standard_name), None)
            if matched_job:
                result = JobMatchResult(
                    query=query,
                    matched_job=matched_job,
                    confidence=1.0,
                    method='synonym_match',
                    alternatives=[],
                    processing_time=(datetime.now() - start_time).total_seconds(),
                    cache_hit=False
                )
                # ìºì‹œ ì €ì¥
                self.redis_client.setex(cache_key, 300, json.dumps(asdict(result)))
                return result
        
        # 2. ì •í™• ì¼ì¹˜
        exact_match = next((job for job in jobs if job['job_title'] == query), None)
        if exact_match:
            result = JobMatchResult(
                query=query,
                matched_job=exact_match,
                confidence=1.0,
                method='exact_match',
                alternatives=[],
                processing_time=(datetime.now() - start_time).total_seconds(),
                cache_hit=False
            )
            self.redis_client.setex(cache_key, 300, json.dumps(asdict(result)))
            return result
        
        # 3. Fuzzy ë§¤ì¹­
        best_matches = await self._fuzzy_search(query, jobs)
        
        if best_matches:
            best_match = best_matches[0]
            alternatives = best_matches[1:5]  # ìƒìœ„ 5ê°œ í›„ë³´
            
            result = JobMatchResult(
                query=query,
                matched_job=best_match['job'],
                confidence=best_match['score'],
                method='fuzzy_match',
                alternatives=[alt['job'] for alt in alternatives],
                processing_time=(datetime.now() - start_time).total_seconds(),
                cache_hit=False
            )
            
            # ë†’ì€ ì‹ ë¢°ë„ì˜ ê²½ìš° ìºì‹œ ì €ì¥
            if best_match['score'] >= 0.8:
                self.redis_client.setex(cache_key, 300, json.dumps(asdict(result)))
            
            return result
        
        # ë§¤ì¹­ ì‹¤íŒ¨
        return JobMatchResult(
            query=query,
            matched_job={},
            confidence=0.0,
            method='no_match',
            alternatives=[],
            processing_time=(datetime.now() - start_time).total_seconds(),
            cache_hit=False
        )
    
    async def _fuzzy_search(self, query: str, jobs: List[Dict]) -> List[Dict]:
        """Fuzzy ê²€ìƒ‰"""
        scores = []
        
        for job in jobs:
            job_title = job['job_title']
            
            # ì—¬ëŸ¬ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°
            levenshtein = 1 - (Levenshtein.distance(query, job_title) / max(len(query), len(job_title)))
            jaro_winkler = Levenshtein.jaro_winkler(query, job_title)
            sequence = SequenceMatcher(None, query, job_title).ratio()
            
            # TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            tfidf_score = 0.0
            if self.job_vectors is not None and job_title in self.job_names:
                query_vector = self.vectorizer.transform([query])
                job_idx = self.job_names.index(job_title)
                job_vector = self.job_vectors[job_idx]
                tfidf_score = cosine_similarity(query_vector, job_vector)[0][0]
            
            # ê°€ì¤‘ í‰ê· 
            combined_score = (
                levenshtein * 0.3 +
                jaro_winkler * 0.3 +
                sequence * 0.2 +
                tfidf_score * 0.2
            )
            
            scores.append({
                'job': job,
                'score': combined_score,
                'details': {
                    'levenshtein': levenshtein,
                    'jaro_winkler': jaro_winkler,
                    'sequence': sequence,
                    'tfidf': tfidf_score
                }
            })
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        scores.sort(key=lambda x: x['score'], reverse=True)
        return scores
    
    def invalidate_cache(self):
        """ìºì‹œ ë¬´íš¨í™”"""
        self.redis_client.delete(self.CACHE_KEY_JOBS)
        self.redis_client.delete(self.CACHE_KEY_SYNONYMS)
        self.logger.info("ì§ë¬´ëª… ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ")


class JobProfileSync:
    """ì§ë¬´ í”„ë¡œí•„ ì‹¤ì‹œê°„ ë™ê¸°í™”"""
    
    def __init__(self, elasticsearch_client=None):
        self.es = elasticsearch_client or Elasticsearch([{'host': 'localhost', 'port': 9200}])
        self.logger = logging.getLogger(self.__class__.__name__)
        self.standardizer = JobProfileStandardizer()
    
    def sync_to_elasticsearch(self, job_profile_id: int) -> SyncResult:
        """Elasticsearch ë™ê¸°í™”"""
        try:
            from job_profiles.models import JobProfile
            
            job = JobProfile.objects.get(id=job_profile_id)
            
            # ê²€ìƒ‰ ë¬¸ì„œ ìƒì„±
            doc = {
                'id': job.id,
                'job_title': job.job_title,
                'job_type': job.job_type,
                'job_category': job.job_category,
                'job_code': job.job_code,
                'description': job.description,
                'requirements': job.requirements,
                'skills': job.skills,
                'is_active': job.is_active,
                'created_at': job.created_at.isoformat(),
                'updated_at': job.updated_at.isoformat(),
                'search_keywords': self._generate_search_keywords(job)
            }
            
            # Elasticsearch ì¸ë±ì‹±
            self.es.index(
                index='job_profiles',
                id=job_profile_id,
                body=doc
            )
            
            self.logger.info(f"ì§ë¬´ í”„ë¡œí•„ {job_profile_id} Elasticsearch ë™ê¸°í™” ì™„ë£Œ")
            
            return SyncResult(
                operation='elasticsearch_sync',
                affected_records=1,
                success=True,
                error_message=None,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"Elasticsearch ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return SyncResult(
                operation='elasticsearch_sync',
                affected_records=0,
                success=False,
                error_message=str(e),
                timestamp=datetime.now()
            )
    
    def _generate_search_keywords(self, job_profile) -> List[str]:
        """ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        keywords = []
        
        # ì§ë¬´ëª… ë¶„í•´
        keywords.extend(job_profile.job_title.split())
        
        # ì§ì¢…, ì§êµ°
        if job_profile.job_type:
            keywords.append(job_profile.job_type)
        if job_profile.job_category:
            keywords.append(job_profile.job_category)
        
        # ìŠ¤í‚¬ í‚¤ì›Œë“œ
        if job_profile.skills:
            skills_list = json.loads(job_profile.skills) if isinstance(job_profile.skills, str) else job_profile.skills
            keywords.extend(skills_list)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(keywords))
    
    async def batch_sync_all(self) -> SyncResult:
        """ì „ì²´ ë°°ì¹˜ ë™ê¸°í™”"""
        try:
            from job_profiles.models import JobProfile
            
            jobs = JobProfile.objects.filter(is_active=True)
            success_count = 0
            
            for job in jobs:
                result = self.sync_to_elasticsearch(job.id)
                if result.success:
                    success_count += 1
            
            # ìºì‹œ ì´ˆê¸°í™”
            self.standardizer.invalidate_cache()
            self.standardizer._initialize_cache()
            
            return SyncResult(
                operation='batch_sync_all',
                affected_records=success_count,
                success=True,
                error_message=None,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return SyncResult(
                operation='batch_sync_all',
                affected_records=0,
                success=False,
                error_message=str(e),
                timestamp=datetime.now()
            )


class JobChatbotService:
    """AI ì§ë¬´ ì±—ë´‡ ì„œë¹„ìŠ¤"""
    
    def __init__(self, openai_api_key: str):
        openai.api_key = openai_api_key
        self.standardizer = JobProfileStandardizer()
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def process_job_query(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        """ì§ë¬´ ê´€ë ¨ ì§ˆì˜ ì²˜ë¦¬"""
        try:
            # 1. ì§ë¬´ëª… ì¶”ì¶œ ë° í‘œì¤€í™”
            job_match = await self.standardizer.find_best_match(query)
            
            # 2. AI ì‘ë‹µ ìƒì„±
            if job_match.confidence >= 0.7:
                # í‘œì¤€í™”ëœ ì§ë¬´ ì •ë³´ë¡œ ìƒì„¸ ì‘ë‹µ
                response = await self._generate_detailed_response(job_match, query, user_context)
            else:
                # ì¼ë°˜ì ì¸ ì§ë¬´ ì •ë³´ ì œê³µ
                response = await self._generate_general_response(query, user_context)
            
            return {
                'success': True,
                'response': response,
                'job_match': asdict(job_match),
                'confidence': job_match.confidence,
                'processing_time': job_match.processing_time
            }
            
        except Exception as e:
            self.logger.error(f"ì±—ë´‡ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'response': "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                'error': str(e)
            }
    
    async def _generate_detailed_response(self, job_match: JobMatchResult, 
                                        query: str, user_context: Dict) -> str:
        """ìƒì„¸ ì§ë¬´ ì •ë³´ ì‘ë‹µ ìƒì„±"""
        job = job_match.matched_job
        
        prompt = f"""
ì‚¬ìš©ìê°€ '{query}'ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
ë§¤ì¹­ëœ ì§ë¬´ ì •ë³´:
- ì§ë¬´ëª…: {job.get('job_title', '')}
- ì§ì¢…: {job.get('job_type', '')}
- ì§êµ°: {job.get('job_category', '')}
- ì„¤ëª…: {job.get('description', '')}
- ìš”êµ¬ì‚¬í•­: {job.get('requirements', '')}

ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸: {user_context or 'ì—†ìŒ'}

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œê·¼í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì§ë¬´ì˜ í•µì‹¬ ì—­í• , í•„ìš” ìŠ¤í‚¬, ë°œì „ ê²½ë¡œ ë“±ì„ í¬í•¨í•´ì„œ ë‹µë³€í•˜ë˜, 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ê²Œ ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.
"""
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œê·¼í•œ HR ì§ë¬´ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    async def _generate_general_response(self, query: str, user_context: Dict) -> str:
        """ì¼ë°˜ ì§ë¬´ ì •ë³´ ì‘ë‹µ ìƒì„±"""
        prompt = f"""
ì‚¬ìš©ìê°€ '{query}'ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
ì •í™•í•œ ì§ë¬´ ë§¤ì¹­ì€ ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì¼ë°˜ì ì¸ ì§ë¬´ ì •ë³´ë‚˜ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸: {user_context or 'ì—†ìŒ'}

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì‹œê³ , 
ë” êµ¬ì²´ì ì¸ ì§ë¬´ëª…ìœ¼ë¡œ ì§ˆë¬¸í•˜ë©´ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤ê³  ì•ˆë‚´í•´ì£¼ì„¸ìš”.
"""
        
        response = await openai.ChatCompletion.acreate(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œê·¼í•œ HR ì§ë¬´ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        return response.choices[0].message.content


class JobSearchService:
    """ì§ë¬´ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(self, elasticsearch_client=None):
        self.es = elasticsearch_client or Elasticsearch([{'host': 'localhost', 'port': 9200}])
        self.standardizer = JobProfileStandardizer()
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def search_jobs(self, query: str, filters: Dict = None, 
                         page: int = 1, page_size: int = 20) -> Dict[str, Any]:
        """ì§ë¬´ ê²€ìƒ‰"""
        try:
            # 1. ê²€ìƒ‰ì–´ í‘œì¤€í™”
            job_match = await self.standardizer.find_best_match(query)
            
            # 2. Elasticsearch ì¿¼ë¦¬ êµ¬ì„±
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": [
                                        "job_title^3",
                                        "job_type^2",
                                        "job_category^2",
                                        "description",
                                        "requirements",
                                        "search_keywords^2"
                                    ],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            }
                        ],
                        "filter": []
                    }
                },
                "highlight": {
                    "fields": {
                        "job_title": {},
                        "description": {},
                        "requirements": {}
                    }
                },
                "from": (page - 1) * page_size,
                "size": page_size,
                "sort": [
                    "_score",
                    {"updated_at": "desc"}
                ]
            }
            
            # 3. í•„í„° ì ìš©
            if filters:
                if filters.get('job_type'):
                    search_body["query"]["bool"]["filter"].append({
                        "term": {"job_type.keyword": filters['job_type']}
                    })
                if filters.get('job_category'):
                    search_body["query"]["bool"]["filter"].append({
                        "term": {"job_category.keyword": filters['job_category']}
                    })
                if filters.get('is_active') is not None:
                    search_body["query"]["bool"]["filter"].append({
                        "term": {"is_active": filters['is_active']}
                    })
            
            # 4. ê²€ìƒ‰ ì‹¤í–‰
            result = self.es.search(
                index='job_profiles',
                body=search_body
            )
            
            # 5. ê²°ê³¼ ê°€ê³µ
            jobs = []
            for hit in result['hits']['hits']:
                job = hit['_source']
                job['score'] = hit['_score']
                job['highlights'] = hit.get('highlight', {})
                jobs.append(job)
            
            return {
                'success': True,
                'jobs': jobs,
                'total': result['hits']['total']['value'],
                'page': page,
                'page_size': page_size,
                'standardized_query': job_match.matched_job.get('job_title', query) if job_match.confidence >= 0.7 else query,
                'suggestions': [alt['job_title'] for alt in job_match.alternatives[:3]],
                'processing_time': job_match.processing_time
            }
            
        except Exception as e:
            self.logger.error(f"ì§ë¬´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'jobs': [],
                'total': 0,
                'error': str(e)
            }
    
    async def get_job_suggestions(self, partial_query: str, limit: int = 10) -> List[str]:
        """ìë™ì™„ì„± ì œì•ˆ"""
        try:
            search_body = {
                "suggest": {
                    "job_suggestions": {
                        "prefix": partial_query,
                        "completion": {
                            "field": "job_title.completion",
                            "size": limit
                        }
                    }
                }
            }
            
            result = self.es.search(
                index='job_profiles',
                body=search_body
            )
            
            suggestions = []
            for suggestion in result['suggest']['job_suggestions'][0]['options']:
                suggestions.append(suggestion['text'])
            
            return suggestions
            
        except Exception as e:
            self.logger.error(f"ìë™ì™„ì„± ì œì•ˆ ì‹¤íŒ¨: {e}")
            return []


# Django Views
@method_decorator(csrf_exempt, name='dispatch')
class JobSearchAPIView(View):
    """ì§ë¬´ ê²€ìƒ‰ API"""
    
    def __init__(self):
        self.search_service = JobSearchService()
    
    async def get(self, request):
        """ì§ë¬´ ê²€ìƒ‰"""
        query = request.GET.get('q', '')
        page = int(request.GET.get('page', 1))
        page_size = int(request.GET.get('page_size', 20))
        
        filters = {}
        if request.GET.get('job_type'):
            filters['job_type'] = request.GET.get('job_type')
        if request.GET.get('job_category'):
            filters['job_category'] = request.GET.get('job_category')
        
        result = await self.search_service.search_jobs(query, filters, page, page_size)
        return JsonResponse(result)
    
    async def post(self, request):
        """ê³ ê¸‰ ê²€ìƒ‰"""
        data = json.loads(request.body)
        query = data.get('query', '')
        filters = data.get('filters', {})
        page = data.get('page', 1)
        page_size = data.get('page_size', 20)
        
        result = await self.search_service.search_jobs(query, filters, page, page_size)
        return JsonResponse(result)


@method_decorator(csrf_exempt, name='dispatch')
class JobChatbotAPIView(View):
    """AI ì§ë¬´ ì±—ë´‡ API"""
    
    def __init__(self):
        self.chatbot_service = JobChatbotService(openai_api_key=os.getenv('OPENAI_API_KEY'))
    
    async def post(self, request):
        """ì±—ë´‡ ì§ˆì˜ ì²˜ë¦¬"""
        data = json.loads(request.body)
        query = data.get('query', '')
        user_context = data.get('context', {})
        
        result = await self.chatbot_service.process_job_query(query, user_context)
        return JsonResponse(result)


@method_decorator(csrf_exempt, name='dispatch')
class JobSyncAPIView(View):
    """ì§ë¬´ ë°ì´í„° ë™ê¸°í™” API"""
    
    def __init__(self):
        self.sync_service = JobProfileSync()
    
    async def post(self, request):
        """ìˆ˜ë™ ë™ê¸°í™” íŠ¸ë¦¬ê±°"""
        data = json.loads(request.body)
        sync_type = data.get('type', 'single')
        
        if sync_type == 'all':
            result = await self.sync_service.batch_sync_all()
        else:
            job_id = data.get('job_id')
            result = self.sync_service.sync_to_elasticsearch(job_id)
        
        return JsonResponse(asdict(result))


# Celery Tasks (commented for demo)
# app = Celery('job_integration')

# @app.task
def sync_job_to_search_engine(job_profile_id: int):
    """ë¹„ë™ê¸° ê²€ìƒ‰ì—”ì§„ ë™ê¸°í™”"""
    sync_service = JobProfileSync()
    result = sync_service.sync_to_elasticsearch(job_profile_id)
    return asdict(result)

# @app.task
def batch_sync_all_jobs():
    """ì „ì²´ ì§ë¬´ ë°°ì¹˜ ë™ê¸°í™”"""
    sync_service = JobProfileSync()
    result = asyncio.run(sync_service.batch_sync_all())
    return asdict(result)


# Django Signal Handlers (commented for demo)
# from django.db.models.signals import post_save, post_delete
# from django.dispatch import receiver

# @receiver(post_save, sender='job_profiles.JobProfile')
def handle_job_profile_save(sender, instance, created, **kwargs):
    """ì§ë¬´ í”„ë¡œí•„ ì €ì¥ ì‹œ ìë™ ë™ê¸°í™”"""
    # ë¹„ë™ê¸° ë™ê¸°í™” ì‹¤í–‰
    # sync_job_to_search_engine.delay(instance.id)
    
    # ìºì‹œ ë¬´íš¨í™”
    standardizer = JobProfileStandardizer()
    # standardizer.invalidate_cache()

# @receiver(post_delete, sender='job_profiles.JobProfile')
def handle_job_profile_delete(sender, instance, **kwargs):
    """ì§ë¬´ í”„ë¡œí•„ ì‚­ì œ ì‹œ ê²€ìƒ‰ì—”ì§„ì—ì„œë„ ì œê±°"""
    try:
        # es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
        # es.delete(index='job_profiles', id=instance.id)
        pass
    except Exception as e:
        logging.getLogger(__name__).error(f"Elasticsearch ì‚­ì œ ì‹¤íŒ¨: {e}")


def create_sample_usage_code():
    """ìƒ˜í”Œ ì‚¬ìš© ì½”ë“œ ìƒì„±"""
    return """
# ì§ë¬´ í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì œ

import asyncio
from job_profile_live_integration import (
    JobProfileStandardizer, JobSearchService, JobChatbotService, JobProfileSync
)

# 1. ì§ë¬´ëª… í‘œì¤€í™”
async def test_standardization():
    standardizer = JobProfileStandardizer()
    
    # ì‹¤ì‹œê°„ ì§ë¬´ ë§¤ì¹­
    result = await standardizer.find_best_match("HRM ë‹´ë‹¹ì")
    print(f"ë§¤ì¹­ ê²°ê³¼: {result.matched_job['job_title']}")
    print(f"ì‹ ë¢°ë„: {result.confidence}")

# 2. ì§ë¬´ ê²€ìƒ‰
async def test_search():
    search_service = JobSearchService()
    
    # ê²€ìƒ‰ ì‹¤í–‰
    results = await search_service.search_jobs(
        query="ì‹œìŠ¤í…œ ê°œë°œ",
        filters={'job_category': 'IT/ë””ì§€í„¸'},
        page=1,
        page_size=10
    )
    
    print(f"ê²€ìƒ‰ ê²°ê³¼: {results['total']}ê°œ")
    for job in results['jobs']:
        print(f"- {job['job_title']} (ì ìˆ˜: {job['score']})")

# 3. AI ì±—ë´‡
async def test_chatbot():
    chatbot = JobChatbotService(openai_api_key="your-api-key")
    
    response = await chatbot.process_job_query(
        query="ì‹œìŠ¤í…œ ê¸°íšìê°€ ë˜ë ¤ë©´ ì–´ë–¤ ìŠ¤í‚¬ì´ í•„ìš”í•œê°€ìš”?",
        user_context={'department': 'IT', 'experience': 'ì‹ ì…'}
    )
    
    print(f"ì±—ë´‡ ì‘ë‹µ: {response['response']}")

# 4. ì‹¤ì‹œê°„ ë™ê¸°í™”
async def test_sync():
    sync_service = JobProfileSync()
    
    # ë‹¨ì¼ ì§ë¬´ ë™ê¸°í™”
    result = sync_service.sync_to_elasticsearch(job_profile_id=1)
    print(f"ë™ê¸°í™” ê²°ê³¼: {result.success}")
    
    # ì „ì²´ ë°°ì¹˜ ë™ê¸°í™”
    batch_result = await sync_service.batch_sync_all()
    print(f"ë°°ì¹˜ ë™ê¸°í™”: {batch_result.affected_records}ê°œ ì²˜ë¦¬")

# 5. Django REST API ì‚¬ìš©
# GET /api/jobs/search/?q=ì‹œìŠ¤í…œê¸°íš&job_type=ITê¸°íš
# POST /api/jobs/chatbot/ {"query": "ë°ì´í„° ë¶„ì„ê°€ ì „ë§", "context": {}}
# POST /api/jobs/sync/ {"type": "all"}

# 6. JavaScript í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™
js_example = '''
// ì§ë¬´ ê²€ìƒ‰ API í˜¸ì¶œ
async function searchJobs(query, filters = {}) {
    const params = new URLSearchParams({ q: query, ...filters });
    const response = await fetch(`/api/jobs/search/?${params}`);
    return await response.json();
}

// AI ì±—ë´‡ API í˜¸ì¶œ
async function askChatbot(query, context = {}) {
    const response = await fetch('/api/jobs/chatbot/', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query, context })
    });
    return await response.json();
}

// ì‹¤ì‹œê°„ ê²€ìƒ‰ ìë™ì™„ì„±
async function getJobSuggestions(partial) {
    const search = new JobSearchService();
    return await search.get_job_suggestions(partial);
}
'''

if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_standardization())
    asyncio.run(test_search())
    # asyncio.run(test_chatbot())  # API í‚¤ í•„ìš”
    asyncio.run(test_sync())
"""


def create_api_specification():
    """API ëª…ì„¸ì„œ ìƒì„±"""
    return """
# ì§ë¬´ í†µí•© ì‹œìŠ¤í…œ API ëª…ì„¸ì„œ

## ê°œìš”
ì§ë¬´ í”„ë¡œí•„ í‘œì¤€í™”, ê²€ìƒ‰, AI ì±—ë´‡ í†µí•© REST API

## ì¸ì¦
```
Authorization: Bearer <token>
Content-Type: application/json
```

## ì—”ë“œí¬ì¸íŠ¸

### 1. ì§ë¬´ ê²€ìƒ‰ API

#### GET /api/jobs/search/
ê¸°ë³¸ ì§ë¬´ ê²€ìƒ‰

**Parameters:**
- `q` (string): ê²€ìƒ‰ì–´
- `page` (int): í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)
- `page_size` (int): í˜ì´ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 20)
- `job_type` (string): ì§ì¢… í•„í„°
- `job_category` (string): ì§êµ° í•„í„°

**Response:**
```json
{
  "success": true,
  "jobs": [
    {
      "id": 1,
      "job_title": "ì‹œìŠ¤í…œê¸°íš",
      "job_type": "ITê¸°íš",
      "job_category": "IT/ë””ì§€í„¸",
      "description": "...",
      "score": 0.95,
      "highlights": {
        "job_title": ["<em>ì‹œìŠ¤í…œ</em>ê¸°íš"]
      }
    }
  ],
  "total": 37,
  "page": 1,
  "page_size": 20,
  "standardized_query": "ì‹œìŠ¤í…œê¸°íš",
  "suggestions": ["ì‹œìŠ¤í…œê°œë°œ", "ì‹œìŠ¤í…œê´€ë¦¬"],
  "processing_time": 0.045
}
```

#### POST /api/jobs/search/
ê³ ê¸‰ ê²€ìƒ‰

**Request Body:**
```json
{
  "query": "ì‹œìŠ¤í…œ ê°œë°œ",
  "filters": {
    "job_type": "ITê°œë°œ",
    "job_category": "IT/ë””ì§€í„¸",
    "is_active": true
  },
  "page": 1,
  "page_size": 10
}
```

### 2. AI ì±—ë´‡ API

#### POST /api/jobs/chatbot/
AI ì§ë¬´ ìƒë‹´

**Request Body:**
```json
{
  "query": "ì‹œìŠ¤í…œ ê¸°íšìê°€ ë˜ë ¤ë©´ ì–´ë–¤ ìŠ¤í‚¬ì´ í•„ìš”í•œê°€ìš”?",
  "context": {
    "department": "IT",
    "experience": "ì‹ ì…",
    "user_id": 123
  }
}
```

**Response:**
```json
{
  "success": true,
  "response": "ì‹œìŠ¤í…œ ê¸°íšìê°€ ë˜ê¸° ìœ„í•´ì„œëŠ”...",
  "job_match": {
    "query": "ì‹œìŠ¤í…œ ê¸°íšì",
    "matched_job": {
      "job_title": "ì‹œìŠ¤í…œê¸°íš",
      "confidence": 0.95
    }
  },
  "confidence": 0.95,
  "processing_time": 1.23
}
```

### 3. ë™ê¸°í™” API

#### POST /api/jobs/sync/
ë°ì´í„° ë™ê¸°í™”

**Request Body:**
```json
{
  "type": "single",  // "single" | "all"
  "job_id": 1        // typeì´ "single"ì¸ ê²½ìš°
}
```

**Response:**
```json
{
  "operation": "elasticsearch_sync",
  "affected_records": 1,
  "success": true,
  "error_message": null,
  "timestamp": "2025-07-26T18:45:00Z"
}
```

### 4. ìë™ì™„ì„± API

#### GET /api/jobs/suggestions/
ì§ë¬´ëª… ìë™ì™„ì„±

**Parameters:**
- `q` (string): ë¶€ë¶„ ê²€ìƒ‰ì–´
- `limit` (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)

**Response:**
```json
{
  "suggestions": [
    "ì‹œìŠ¤í…œê¸°íš",
    "ì‹œìŠ¤í…œê°œë°œ",
    "ì‹œìŠ¤í…œê´€ë¦¬"
  ]
}
```

## ì—ëŸ¬ ì‘ë‹µ

**400 Bad Request:**
```json
{
  "success": false,
  "error": "Invalid query parameter",
  "details": "Query parameter 'q' is required"
}
```

**500 Internal Server Error:**
```json
{
  "success": false,
  "error": "Internal server error",
  "details": "Elasticsearch connection failed"
}
```

## ì‚¬ìš© ì˜ˆì œ

### JavaScript
```javascript
// ì§ë¬´ ê²€ìƒ‰
const searchJobs = async (query) => {
  const response = await fetch(`/api/jobs/search/?q=${encodeURIComponent(query)}`);
  return await response.json();
};

// AI ì±—ë´‡
const askChatbot = async (query, context) => {
  const response = await fetch('/api/jobs/chatbot/', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ query, context })
  });
  return await response.json();
};
```

### Python
```python
import requests

# ì§ë¬´ ê²€ìƒ‰
response = requests.get('/api/jobs/search/', params={'q': 'ì‹œìŠ¤í…œê¸°íš'})
jobs = response.json()

# AI ì±—ë´‡
response = requests.post('/api/jobs/chatbot/', json={
  'query': 'ë°ì´í„° ë¶„ì„ê°€ê°€ ë˜ë ¤ë©´?',
  'context': {'experience': 'ì‹ ì…'}
})
chatbot_response = response.json()
```
"""


def create_configuration_files():
    """ì„¤ì • íŒŒì¼ë“¤ ìƒì„±"""
    files = {}
    
    # Django ì„¤ì •
    files['settings_integration.py'] = """
# Django í†µí•© ì„¤ì •

# Redis ì„¤ì •
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/1',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        }
    }
}

# Celery ì„¤ì •
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'
CELERY_RESULT_SERIALIZER = 'json'
CELERY_TIMEZONE = 'Asia/Seoul'

# Elasticsearch ì„¤ì •
ELASTICSEARCH_DSL = {
    'default': {
        'hosts': 'localhost:9200'
    },
}

# OpenAI ì„¤ì •
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# ì§ë¬´ í†µí•© ì‹œìŠ¤í…œ ì„¤ì •
JOB_INTEGRATION = {
    'CACHE_TTL': 3600,
    'SEARCH_PAGE_SIZE': 20,
    'FUZZY_MATCH_THRESHOLD': 0.7,
    'CHATBOT_MAX_TOKENS': 500,
    'SYNC_BATCH_SIZE': 100
}

# ì¶”ê°€ëœ ì•±
INSTALLED_APPS += [
    'django_redis',
    'elasticsearch_dsl',
    'job_profiles',
]

# ë¡œê¹… ì„¤ì •
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',
            'style': '{',
        },
    },
    'handlers': {
        'file': {
            'level': 'INFO',
            'class': 'logging.FileHandler',
            'filename': 'job_integration.log',
            'formatter': 'verbose',
        },
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'verbose',
        },
    },
    'loggers': {
        'JobProfileStandardizer': {
            'handlers': ['file', 'console'],
            'level': 'INFO',
            'propagate': False,
        },
        'JobSearchService': {
            'handlers': ['file', 'console'],
            'level': 'INFO',
            'propagate': False,
        },
        'JobChatbotService': {
            'handlers': ['file', 'console'],
            'level': 'INFO',
            'propagate': False,
        },
    },
}
"""

    # Celery ì„¤ì •
    files['celery_config.py'] = """
# Celery ì„¤ì • íŒŒì¼

import os
from celery import Celery

# Django ì„¤ì • ëª¨ë“ˆ ì§€ì •
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ehr_system.settings')

# Celery ì•± ìƒì„±
app = Celery('ehr_system')

# Django ì„¤ì •ì—ì„œ êµ¬ì„± ë¡œë“œ
app.config_from_object('django.conf:settings', namespace='CELERY')

# Django ì•±ì—ì„œ íƒœìŠ¤í¬ ìë™ ë°œê²¬
app.autodiscover_tasks()

# ì£¼ê¸°ì  íƒœìŠ¤í¬ ì„¤ì •
from celery.schedules import crontab

app.conf.beat_schedule = {
    'sync-all-jobs-daily': {
        'task': 'job_profile_live_integration.batch_sync_all_jobs',
        'schedule': crontab(hour=2, minute=0),  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    },
    'cleanup-cache-hourly': {
        'task': 'job_profile_live_integration.cleanup_expired_cache',
        'schedule': crontab(minute=0),  # ë§¤ì‹œ ì •ê°
    },
}
"""

    # Elasticsearch ë§¤í•‘
    files['elasticsearch_mapping.json'] = """{
  "mappings": {
    "properties": {
      "id": {
        "type": "integer"
      },
      "job_title": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword"
          },
          "completion": {
            "type": "completion",
            "contexts": [
              {
                "name": "job_category",
                "type": "category"
              }
            ]
          }
        },
        "analyzer": "korean"
      },
      "job_type": {
        "type": "keyword"
      },
      "job_category": {
        "type": "keyword"
      },
      "job_code": {
        "type": "keyword"
      },
      "description": {
        "type": "text",
        "analyzer": "korean"
      },
      "requirements": {
        "type": "text",
        "analyzer": "korean"
      },
      "skills": {
        "type": "keyword"
      },
      "search_keywords": {
        "type": "keyword"
      },
      "is_active": {
        "type": "boolean"
      },
      "created_at": {
        "type": "date"
      },
      "updated_at": {
        "type": "date"
      }
    }
  },
  "settings": {
    "analysis": {
      "analyzer": {
        "korean": {
          "tokenizer": "nori_tokenizer",
          "filter": ["lowercase", "nori_part_of_speech"]
        }
      }
    },
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}"""

    # requirements.txt
    files['requirements.txt'] = """
# ì§ë¬´ í†µí•© ì‹œìŠ¤í…œ í•„ìˆ˜ íŒ¨í‚¤ì§€

# Django ê´€ë ¨
django>=4.2.0
djangorestframework>=3.14.0
django-cors-headers>=4.0.0

# ë°ì´í„°ë² ì´ìŠ¤
psycopg2-binary>=2.9.0

# ìºì‹œ ë° ì„¸ì…˜
redis>=4.5.0
django-redis>=5.2.0

# ê²€ìƒ‰ì—”ì§„
elasticsearch>=8.0.0
elasticsearch-dsl>=8.0.0

# ë¹„ë™ê¸° ì²˜ë¦¬
celery>=5.3.0
kombu>=5.3.0

# AI ë° ìì—°ì–´ ì²˜ë¦¬
openai>=0.27.0
python-Levenshtein>=0.21.0

# ë°ì´í„° ì²˜ë¦¬
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# ìœ í‹¸ë¦¬í‹°
requests>=2.31.0
python-dateutil>=2.8.0
pytz>=2023.3

# ëª¨ë‹ˆí„°ë§
sentry-sdk>=1.32.0

# ê°œë°œ ë„êµ¬ (ì„ íƒì‚¬í•­)
django-debug-toolbar>=4.1.0
pytest>=7.4.0
pytest-django>=4.5.0
black>=23.7.0
"""

    # Docker ì„¤ì •
    files['docker-compose.yml'] = """
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data

  web:
    build: .
    command: python manage.py runserver 0.0.0.0:8000
    volumes:
      - .:/code
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - elasticsearch
    environment:
      - DJANGO_SETTINGS_MODULE=ehr_system.settings
      - REDIS_URL=redis://redis:6379/0
      - ELASTICSEARCH_URL=http://elasticsearch:9200

  celery:
    build: .
    command: celery -A ehr_system worker -l info
    volumes:
      - .:/code
    depends_on:
      - redis
      - elasticsearch
    environment:
      - DJANGO_SETTINGS_MODULE=ehr_system.settings
      - REDIS_URL=redis://redis:6379/0

  celery-beat:
    build: .
    command: celery -A ehr_system beat -l info
    volumes:
      - .:/code
    depends_on:
      - redis
    environment:
      - DJANGO_SETTINGS_MODULE=ehr_system.settings
      - REDIS_URL=redis://redis:6379/0

volumes:
  redis_data:
  es_data:
"""
    
    return files


def create_deployment_guide():
    """ë°°í¬ ê°€ì´ë“œ ìƒì„±"""
    return """
# ì§ë¬´ í†µí•© ì‹œìŠ¤í…œ ë°°í¬ ê°€ì´ë“œ

## 1. í™˜ê²½ ì„¤ì •

### Redis ì„¤ì¹˜ ë° ì„¤ì •
```bash
# Redis ì„¤ì¹˜
sudo apt install redis-server

# Redis ì„¤ì • (/etc/redis/redis.conf)
bind 127.0.0.1
port 6379
maxmemory 256mb
maxmemory-policy allkeys-lru
```

### Elasticsearch ì„¤ì¹˜ ë° ì„¤ì •
```bash
# Elasticsearch ì„¤ì¹˜
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list
sudo apt update && sudo apt install elasticsearch

# ì¸ë±ìŠ¤ ìƒì„±
curl -X PUT "localhost:9200/job_profiles" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "job_title": {
        "type": "text",
        "fields": {
          "keyword": {"type": "keyword"},
          "completion": {"type": "completion"}
        }
      },
      "job_type": {"type": "keyword"},
      "job_category": {"type": "keyword"},
      "description": {"type": "text"},
      "requirements": {"type": "text"},
      "search_keywords": {"type": "keyword"},
      "is_active": {"type": "boolean"},
      "created_at": {"type": "date"},
      "updated_at": {"type": "date"}
    }
  }
}'
```

### Celery ì„¤ì •
```python
# settings.py
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
CELERY_ACCEPT_CONTENT = ['json']
CELERY_TASK_SERIALIZER = 'json'
CELERY_RESULT_SERIALIZER = 'json'

# celery.py
from celery import Celery
import os

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ehr_system.settings')

app = Celery('ehr_system')
app.config_from_object('django.conf:settings', namespace='CELERY')
app.autodiscover_tasks()
```

## 2. Django ì„¤ì •

### URL íŒ¨í„´
```python
# urls.py
from job_profile_live_integration import (
    JobSearchAPIView, JobChatbotAPIView, JobSyncAPIView
)

urlpatterns = [
    path('api/jobs/search/', JobSearchAPIView.as_view(), name='job_search'),
    path('api/jobs/chatbot/', JobChatbotAPIView.as_view(), name='job_chatbot'),
    path('api/jobs/sync/', JobSyncAPIView.as_view(), name='job_sync'),
]
```

### ëª¨ë¸ ì—…ë°ì´íŠ¸
```python
# models.py
class JobSynonym(models.Model):
    synonym = models.CharField(max_length=100, unique=True)
    standard_name = models.CharField(max_length=100)
    is_active = models.BooleanField(default=True)
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        db_table = 'job_synonyms'
```

## 3. ëª¨ë‹ˆí„°ë§ ì„¤ì •

### ë¡œê·¸ ì„¤ì •
```python
# settings.py
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'file': {
            'level': 'INFO',
            'class': 'logging.FileHandler',
            'filename': 'job_integration.log',
        },
    },
    'loggers': {
        'JobProfileStandardizer': {
            'handlers': ['file'],
            'level': 'INFO',
        },
        'JobSearchService': {
            'handlers': ['file'],
            'level': 'INFO',
        },
    },
}
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
# monitoring.py
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        
        # ì„±ëŠ¥ ë¡œê·¸
        logger.info(f"{func.__name__} ì‹¤í–‰ì‹œê°„: {end_time - start_time:.3f}ì´ˆ")
        return result
    return wrapper
```

## 4. ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Redis ì„œë²„ ì‹¤í–‰ í™•ì¸
- [ ] Elasticsearch ì„œë²„ ì‹¤í–‰ í™•ì¸
- [ ] Celery worker ì‹¤í–‰ í™•ì¸
- [ ] ì´ˆê¸° ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰
- [ ] API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
- [ ] ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ì±—ë´‡ ì‘ë‹µ í’ˆì§ˆ í™•ì¸
- [ ] ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ë°±ì—… ìŠ¤ì¼€ì¤„ ì„¤ì •

## 5. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤
1. Redis ì—°ê²° ì‹¤íŒ¨ â†’ Redis ì„œë²„ ìƒíƒœ í™•ì¸
2. Elasticsearch ì¸ë±ì‹± ì‹¤íŒ¨ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ í™•ì¸
3. ê²€ìƒ‰ ì„±ëŠ¥ ì €í•˜ â†’ ì¸ë±ìŠ¤ ìµœì í™” ì‹¤í–‰
4. ì±—ë´‡ ì‘ë‹µ ì§€ì—° â†’ OpenAI API ì œí•œ í™•ì¸
5. ë™ê¸°í™” ì‹¤íŒ¨ â†’ Celery ì›Œì»¤ ìƒíƒœ í™•ì¸
"""


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    output_dir = r"C:/Users/apro/OneDrive/Desktop/EHR_V1.0/job_integration_output"
    os.makedirs(output_dir, exist_ok=True)
    
    print("ğŸš€ ì§ë¬´ í”„ë¡œí•„ ì‹¤ì‹œê°„ í†µí•© ì‹œìŠ¤í…œ ìƒì„± ì¤‘...")
    
    # ìƒ˜í”Œ ì½”ë“œ ìƒì„±
    sample_code = create_sample_usage_code()
    with open(os.path.join(output_dir, 'sample_usage.py'), 'w', encoding='utf-8') as f:
        f.write(sample_code)
    print("âœ… ìƒ˜í”Œ ì‚¬ìš© ì½”ë“œ ìƒì„± ì™„ë£Œ")
    
    # ë°°í¬ ê°€ì´ë“œ ìƒì„±
    deployment_guide = create_deployment_guide()
    with open(os.path.join(output_dir, 'DEPLOYMENT_GUIDE.md'), 'w', encoding='utf-8') as f:
        f.write(deployment_guide)
    print("âœ… ë°°í¬ ê°€ì´ë“œ ìƒì„± ì™„ë£Œ")
    
    # API ëª…ì„¸ì„œ ìƒì„±
    api_spec = create_api_specification()
    with open(os.path.join(output_dir, 'API_SPECIFICATION.md'), 'w', encoding='utf-8') as f:
        f.write(api_spec)
    print("âœ… API ëª…ì„¸ì„œ ìƒì„± ì™„ë£Œ")
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    config_files = create_configuration_files()
    for filename, content in config_files.items():
        with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as f:
            f.write(content)
    print("âœ… ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")
    
    print("\nâœ… ì§ë¬´ í”„ë¡œí•„ ì‹¤ì‹œê°„ í†µí•© ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")
    print("\nğŸš€ ì£¼ìš” ê¸°ëŠ¥:")
    print("  - ì‹¤ì‹œê°„ ì§ë¬´ëª… í‘œì¤€í™” ë° ë§¤ì¹­ (Levenshtein, Jaro-Winkler)")
    print("  - AI ì±—ë´‡ê³¼ ê²€ìƒ‰ì—”ì§„ í†µí•© (OpenAI GPT)")
    print("  - Redis/Elasticsearch ê¸°ë°˜ ê³ ì„±ëŠ¥ ê²€ìƒ‰")
    print("  - ìë™ ë™ê¸°í™” ë° ìºì‹œ ê´€ë¦¬")
    print("  - Django REST API ì œê³µ")
    print("  - Celery ë¹„ë™ê¸° ì²˜ë¦¬")
    print("  - ì‹¤ì‹œê°„ ì–‘ë°©í–¥ ì—°ë™")
    print("\nğŸ“‹ ìƒì„±ëœ íŒŒì¼:")
    print("  - job_profile_live_integration.py (ë©”ì¸ í†µí•© ì‹œìŠ¤í…œ)")
    print("  - sample_usage.py (ì‚¬ìš© ì˜ˆì œ)")
    print("  - DEPLOYMENT_GUIDE.md (ë°°í¬ ê°€ì´ë“œ)")
    print("  - API_SPECIFICATION.md (API ëª…ì„¸ì„œ)")
    print("  - settings_integration.py (Django ì„¤ì •)")
    print("  - celery_config.py (Celery ì„¤ì •)")
    print("  - elasticsearch_mapping.json (ES ì¸ë±ìŠ¤ ë§¤í•‘)")
    print("\nğŸ”§ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. Redis, Elasticsearch ì„¤ì¹˜")
    print("  2. Django ì„¤ì • ì—…ë°ì´íŠ¸")
    print("  3. pip install -r requirements.txt")
    print("  4. ì´ˆê¸° ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰")
    print("  5. API í…ŒìŠ¤íŠ¸ ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •")


if __name__ == '__main__':
    main()